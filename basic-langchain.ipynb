{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { load } from \"dotenv\";\n",
    "const env = await load();\n",
    "\n",
    "const process = {\n",
    "    env\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"Why don't scientists trust atoms? \\n\\nBecause they make up everything!\"\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "  content: \u001b[32m\"Why don't scientists trust atoms? \\n\\nBecause they make up everything!\"\u001b[39m,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "  response_metadata: {\n",
       "    tokenUsage: { completionTokens: \u001b[33m14\u001b[39m, promptTokens: \u001b[33m11\u001b[39m, totalTokens: \u001b[33m25\u001b[39m },\n",
       "    finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const model = new ChatOpenAI();\n",
    "\n",
    "await model.invoke([\n",
    "    new HumanMessage(\"Tell me a joke\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序曲\n",
      "　　今天是我的生日，直到晚上爸爸妈妈点上了生日蛋糕的蜡烛，我们三个围着十四个小火苗坐下来，我才想起这事。\n",
      "　　这是个雷雨之夜，整个宇宙似乎是由密集的闪电和我们的小屋组成。当那蓝色的电光闪起时，窗外的雨珠在一瞬间看得清清楚楚，那雨珠似乎凝固了，像密密地挂在天地间的一串串晶莹的水晶。这时我的脑海中就有一个闪念：世界要是那样的也很有意思，你每天一出门，就在那水晶的密帘中走路，它们在你周围发出丁零丁零的响声，只是，这样玲珑剔透的世界，如何经得住那暴烈的雷电呢……世界在我的眼中总和在别人眼中不一样，我总是努力使世界变形，这是我长这么大对自己唯一的认识。\n",
      "　　暴雨是从傍晚开始的，自那以后闪电和雷声越来越密，开始，每当一道闪电过后，我脑海中一边回忆着刚才窗外那转瞬即逝的水晶世界，一边绷紧头皮等待着那一声炸雷，但现在，闪电太密集了，我已分不出哪声雷属于哪个闪电了。\n"
     ]
    }
   ],
   "source": [
    "import { TextLoader } from \"langchain/document_loaders/fs/text\";\n",
    "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
    "  \n",
    "const loader = new TextLoader(\"data/qiu.txt\");\n",
    "const docs = await loader.load();\n",
    "const splitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 500,\n",
    "    chunkOverlap: 100,\n",
    "});\n",
    "\n",
    "const splitDocs = await splitter.splitDocuments(docs);\n",
    "console.log(splitDocs[4].pageContent)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Why couldn't the bicycle stand up by itself? \\n\\nBecause it was two-tired!\"\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const chatModel = new ChatOpenAI();\n",
    "const outputPrase = new StringOutputParser();\n",
    "\n",
    "const simpleChain = chatModel.pipe(outputPrase)\n",
    "\n",
    "await simpleChain.invoke([\n",
    "    new HumanMessage(\"Tell me a joke\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  \u001b[32m\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\"\u001b[39m,\n",
       "  \u001b[32m\"Hello! I am an AI digital assistant here to help answer your questions and provide information on a wide range of topics. How can I assist you today?\"\u001b[39m\n",
       "]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const chatModel = new ChatOpenAI();\n",
    "const outputPrase = new StringOutputParser();\n",
    "\n",
    "const simpleChain = chatModel.pipe(outputPrase)\n",
    "\n",
    "await simpleChain.batch([\n",
    "    [ new HumanMessage(\"Tell me a joke\") ],\n",
    "    [ new HumanMessage(\"Hi, Who are you?\") ],\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " did\n",
      " the\n",
      " scare\n",
      "crow\n",
      " win\n",
      " an\n",
      " award\n",
      "?\n",
      "\n",
      "Because\n",
      " he\n",
      " was\n",
      " outstanding\n",
      " in\n",
      " his\n",
      " field\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const chatModel = new ChatOpenAI();\n",
    "const outputPrase = new StringOutputParser();\n",
    "\n",
    "const stream = await simpleChain.stream([\n",
    "     new HumanMessage(\"Tell me a joke\")\n",
    "])\n",
    "\n",
    "for await (const chunk of stream){\n",
    "    console.log(chunk)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"replace\",\n",
      "      path: \"\",\n",
      "      value: {\n",
      "        id: \"03193401-16fc-4bf4-99f0-7cb000b8ad31\",\n",
      "        name: \"RunnableSequence\",\n",
      "        type: \"chain\",\n",
      "        streamed_output: [],\n",
      "        final_output: undefined,\n",
      "        logs: {}\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI\",\n",
      "      value: {\n",
      "        id: \"b775c2e4-d16a-42b2-a7a8-d43f677d6e38\",\n",
      "        name: \"ChatOpenAI\",\n",
      "        type: \"llm\",\n",
      "        tags: [ \"seq:step:1\" ],\n",
      "        metadata: {},\n",
      "        start_time: \"2025-06-01T17:58:10.123Z\",\n",
      "        streamed_output: [],\n",
      "        streamed_output_str: [],\n",
      "        final_output: undefined,\n",
      "        end_time: undefined\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser\",\n",
      "      value: {\n",
      "        id: \"c2ad5afd-6014-4023-a952-59b4e4a5e80d\",\n",
      "        name: \"StrOutputParser\",\n",
      "        type: \"parser\",\n",
      "        tags: [ \"seq:step:2\" ],\n",
      "        metadata: {},\n",
      "        start_time: \"2025-06-01T17:58:11.013Z\",\n",
      "        streamed_output: [],\n",
      "        streamed_output_str: [],\n",
      "        final_output: undefined,\n",
      "        end_time: undefined\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \"\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \"\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \"\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \"\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \"\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \"Why\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \"Why\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \"Why\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \"Why\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \"Why\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" did\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" did\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" did\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" did\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" did\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" the\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" the\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" the\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" the\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" the\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" scare\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" scare\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" scare\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" scare\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" scare\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \"crow\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \"crow\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \"crow\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \"crow\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \"crow\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" win\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" win\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" win\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" win\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" win\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" an\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" an\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" an\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" an\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" an\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" award\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" award\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" award\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" award\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" award\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \"?\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \"?\\n\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \"?\\n\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \"?\\n\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \"?\\n\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \"Because\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \"Because\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \"Because\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \"Because\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \"Because\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" he\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" he\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" he\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" he\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" he\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" was\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" was\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" was\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" was\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" was\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" outstanding\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" outstanding\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" outstanding\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" outstanding\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" outstanding\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" in\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" in\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" in\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" in\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" in\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" his\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" his\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" his\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" his\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" his\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \" field\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \" field\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \" field\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \" field\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \" field\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \"!\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \"!\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \"!\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \"!\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \"!\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/streamed_output/-\",\n",
      "      value: \"\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [ { op: \"add\", path: \"/streamed_output/-\", value: \"\" } ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output_str/-\",\n",
      "      value: \"\"\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/streamed_output/-\",\n",
      "      value: ChatGenerationChunk {\n",
      "        text: \"\",\n",
      "        generationInfo: { prompt: 0, completion: 0, finish_reason: \"stop\" },\n",
      "        message: AIMessageChunk {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_namespace: [Array],\n",
      "          content: \"\",\n",
      "          name: undefined,\n",
      "          additional_kwargs: {},\n",
      "          response_metadata: [Object]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/final_output\",\n",
      "      value: { generations: [ [Array] ] }\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/ChatOpenAI/end_time\",\n",
      "      value: \"2025-06-01T17:58:11.220Z\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/final_output\",\n",
      "      value: {\n",
      "        output: \"Why did the scarecrow win an award?\\n\" +\n",
      "          \"Because he was outstanding in his field!\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      op: \"add\",\n",
      "      path: \"/logs/StrOutputParser/end_time\",\n",
      "      value: \"2025-06-01T17:58:11.221Z\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "RunLogPatch {\n",
      "  ops: [\n",
      "    {\n",
      "      op: \"replace\",\n",
      "      path: \"/final_output\",\n",
      "      value: {\n",
      "        output: \"Why did the scarecrow win an award?\\n\" +\n",
      "          \"Because he was outstanding in his field!\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const chatModel = new ChatOpenAI();\n",
    "const outputPrase = new StringOutputParser();\n",
    "\n",
    "const stream = await simpleChain.streamLog([\n",
    "     new HumanMessage(\"Tell me a joke\")\n",
    "])\n",
    "\n",
    "for await (const chunk of stream){\n",
    "    console.log(chunk)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"你好！有什么可以帮助你的吗？如果有任何问题，请随时向我提问。\"\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "  content: \u001b[32m\"你好！有什么可以帮助你的吗？如果有任何问题，请随时向我提问。\"\u001b[39m,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "  response_metadata: {\n",
       "    tokenUsage: { completionTokens: \u001b[33m31\u001b[39m, promptTokens: \u001b[33m9\u001b[39m, totalTokens: \u001b[33m40\u001b[39m },\n",
       "    finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const fakeLLM = new ChatOpenAI({\n",
    "    configuration: {\n",
    "        baseURL: \"https://api.openai.com/v12\",\n",
    "    },\n",
    "    maxRetries: 0,\n",
    "});\n",
    "\n",
    "// await fakeLLM.invoke(\"你好\")\n",
    "\n",
    "const realLLM = new ChatOpenAI({\n",
    "  // configuration: {\n",
    "  //   baseURL: \"https://api.openai.com/v1\",\n",
    "  // }\n",
    "})\n",
    "const llmWithFallback = fakeLLM.withFallbacks({\n",
    "    fallbacks: [realLLM]\n",
    "})\n",
    "\n",
    "await llmWithFallback.invoke(\"你好\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "const greetingPrompt = new PromptTemplate({\n",
    "  inputVariables: [],\n",
    "  template: \"hello world\",\n",
    "});\n",
    "const formattedGreetingPrompt = await greetingPrompt.format();\n",
    "\n",
    "console.log(formattedGreetingPrompt);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello，Tunan\n"
     ]
    }
   ],
   "source": [
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "const personalizedGreetingPrompt = new PromptTemplate({\n",
    "  inputVariables: [\"name\"],\n",
    "  template: \"hello，{name}\",\n",
    "});\n",
    "const formattedPersonalizedGreeting = await personalizedGreetingPrompt.format({\n",
    "  name: \"Tunan\",\n",
    "});\n",
    "\n",
    "console.log(formattedPersonalizedGreeting);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good night, Tunan\n"
     ]
    }
   ],
   "source": [
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "const multiVariableGreetingPrompt = new PromptTemplate({\n",
    "  inputVariables: [\"timeOfDay\", \"name\"],\n",
    "  template: \"good {timeOfDay}, {name}\",\n",
    "});\n",
    "const formattedMultiVariableGreeting = await multiVariableGreetingPrompt.format({\n",
    "  timeOfDay: \"night\",\n",
    "  name: \"Tunan\",\n",
    "});\n",
    "\n",
    "console.log(formattedMultiVariableGreeting);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"timeOfDay\", \"name\" ]\n",
      "good morning, Tunan\n"
     ]
    }
   ],
   "source": [
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "const autoInferTemplate = PromptTemplate.fromTemplate(\"good {timeOfDay}, {name}\");\n",
    "console.log(autoInferTemplate.inputVariables);\n",
    "// ['timeOfDay', 'name']\n",
    "\n",
    "const formattedAutoInferTemplate = await autoInferTemplate.format({\n",
    "  timeOfDay: \"morning\",\n",
    "  name: \"Tunan\",\n",
    "});\n",
    "console.log(formattedAutoInferTemplate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个工具，它是锤子。\n",
      "这是一个工具，它是改锥。\n"
     ]
    }
   ],
   "source": [
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "const initialPrompt = new PromptTemplate({\n",
    "  template: \"这是一个{type}，它是{item}。\",\n",
    "  inputVariables: [\"type\", \"item\"],\n",
    "});\n",
    "\n",
    "\n",
    "const partialedPrompt = await initialPrompt.partial({\n",
    "  type: \"工具\",\n",
    "});\n",
    "\n",
    "const formattedPrompt = await partialedPrompt.format({\n",
    "  item: \"锤子\",\n",
    "});\n",
    "\n",
    "console.log(formattedPrompt);\n",
    "\n",
    "const formattedPrompt2 = await partialedPrompt.format({\n",
    "  item: \"改锥\",\n",
    "});\n",
    "\n",
    "console.log(formattedPrompt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天是6/1/2025，我们去爬山。\n"
     ]
    }
   ],
   "source": [
    "const getCurrentDateStr = () => {\n",
    "  return new Date().toLocaleDateString();\n",
    "};\n",
    "\n",
    "const promptWithDate = new PromptTemplate({\n",
    "  template: \"今天是{date}，{activity}。\",\n",
    "  inputVariables: [\"date\", \"activity\"],\n",
    "});\n",
    "\n",
    "const partialedPromptWithDate = await promptWithDate.partial({\n",
    "  date: getCurrentDateStr,\n",
    "});\n",
    "\n",
    "const formattedPromptWithDate = await partialedPromptWithDate.format({\n",
    "  activity: \"我们去爬山\",\n",
    "});\n",
    "\n",
    "console.log(formattedPromptWithDate);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/1/2025 下午好!\n"
     ]
    }
   ],
   "source": [
    "const getCurrentDateStr = () => {\n",
    "  return new Date().toLocaleDateString();\n",
    "};\n",
    "\n",
    "function generateGreeting(timeOfDay) {\n",
    "  return () => {\n",
    "    const date = getCurrentDateStr()\n",
    "    switch (timeOfDay) {\n",
    "      case 'morning':\n",
    "        return date + ' 早上好';\n",
    "      case 'afternoon':\n",
    "        return date + ' 下午好';\n",
    "      case 'evening':\n",
    "        return date + ' 晚上好';\n",
    "      default:\n",
    "        return date + ' 你好';\n",
    "    }\n",
    "  };\n",
    "}\n",
    "\n",
    "const prompt = new PromptTemplate({\n",
    "  template: \"{greeting}!\",\n",
    "  inputVariables: [\"greeting\"],\n",
    "});\n",
    "\n",
    "const currentTimeOfDay = 'afternoon';\n",
    "const partialPrompt = await prompt.partial({\n",
    "  greeting: generateGreeting(currentTimeOfDay),\n",
    "});\n",
    "\n",
    "const formattedPrompt = await partialPrompt.format();\n",
    "\n",
    "console.log(formattedPrompt);\n",
    "// 输出: 3/21/2024 下午好!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  SystemMessage {\n",
      "    lc_serializable: true,\n",
      "    lc_kwargs: {\n",
      "      content: \"你是一个专\\n业的翻译员，你的任务是将文本从中文翻译成法语。\",\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "    content: \"你是一个专\\n业的翻译员，你的任务是将文本从中文翻译成法语。\",\n",
      "    name: undefined,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  HumanMessage {\n",
      "    lc_serializable: true,\n",
      "    lc_kwargs: {\n",
      "      content: \"请翻译这句话：你好，世界\",\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "    content: \"请翻译这句话：你好，世界\",\n",
      "    name: undefined,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "const translateInstructionTemplate = SystemMessagePromptTemplate.fromTemplate(`你是一个专\n",
    "业的翻译员，你的任务是将文本从{source_lang}翻译成{target_lang}。`);\n",
    "\n",
    "const userQuestionTemplate = HumanMessagePromptTemplate.fromTemplate(\"请翻译这句话：{text}\")\n",
    "\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "  translateInstructionTemplate,\n",
    "  userQuestionTemplate,\n",
    "]);\n",
    "\n",
    "const formattedChatPrompt = await chatPrompt.formatMessages({\n",
    "  source_lang: \"中文\",\n",
    "  target_lang: \"法语\",\n",
    "  text: \"你好，世界\",\n",
    "});\n",
    "\n",
    "console.log(formattedChatPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Bonjour, le monde.\"\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { load } from \"dotenv\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const systemTemplate = \"你是一个专业的翻译员，你的任务是将文本从{source_lang}翻译成{target_lang}。\";\n",
    "const humanTemplate = \"请翻译这句话：{text}\";\n",
    "\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", systemTemplate],\n",
    "  [\"human\", humanTemplate],\n",
    "]);\n",
    "\n",
    "const model = new ChatOpenAI();\n",
    "const outputParser = new StringOutputParser();\n",
    "\n",
    "const chain = chatPrompt.pipe(model).pipe(outputParser);\n",
    "\n",
    "await chain.invoke({\n",
    "  source_lang: \"中文\",\n",
    "  target_lang: \"法语\",\n",
    "  text: \"你好，世界\",\n",
    "});\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "你是一个智能管家，今天是 6/1/2025，现在是 晚上，你的主人的信息是姓名是 张三, 性别是 male, \n",
      "根据上下文，完成主人的需求\n",
      "\n",
      "我想吃 晚上 的 lemon。 \n",
      "再重复一遍我的信息 姓名是 张三, 性别是 male\n"
     ]
    }
   ],
   "source": [
    "import {\n",
    "  PromptTemplate,\n",
    "  PipelinePromptTemplate,\n",
    "} from \"@langchain/core/prompts\";\n",
    "\n",
    "const getCurrentDateStr = () => {\n",
    "  return new Date().toLocaleDateString();\n",
    "};\n",
    "\n",
    "const fullPrompt = PromptTemplate.fromTemplate(`\n",
    "你是一个智能管家，今天是 {date}，你的主人的信息是{info}, \n",
    "根据上下文，完成主人的需求\n",
    "{task}`);\n",
    "\n",
    "const datePrompt = PromptTemplate.fromTemplate(\"{date}，现在是 {period}\")\n",
    "const periodPrompt = await datePrompt.partial({\n",
    "    date: getCurrentDateStr\n",
    "})\n",
    "\n",
    "const infoPrompt = PromptTemplate.fromTemplate(\"姓名是 {name}, 性别是 {gender}\");\n",
    "\n",
    "const taskPrompt = PromptTemplate.fromTemplate(`\n",
    "我想吃 {period} 的 {food}。 \n",
    "再重复一遍我的信息 {info}`);\n",
    "\n",
    "\n",
    "const composedPrompt = await new PipelinePromptTemplate({\n",
    "  pipelinePrompts: [\n",
    "    {\n",
    "      name: \"date\",\n",
    "      prompt: periodPrompt,\n",
    "    },\n",
    "    {\n",
    "      name: \"info\",\n",
    "      prompt: infoPrompt,\n",
    "    },\n",
    "    {\n",
    "      name: \"task\",\n",
    "      prompt: taskPrompt,\n",
    "    },\n",
    "  ],\n",
    "  finalPrompt: fullPrompt,\n",
    "})\n",
    "\n",
    "const formattedPrompt = await composedPrompt.format({\n",
    "  period: '晚上',\n",
    "  name: \"张三\",\n",
    "  gender: \"male\",\n",
    "  food: \"lemon\"\n",
    "})\n",
    "\n",
    "console.log(formattedPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"Why did the math book look sad?\\n\\nBecause it had too many problems.\"\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "  content: \u001b[32m\"Why did the math book look sad?\\n\\nBecause it had too many problems.\"\u001b[39m,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "  response_metadata: {\n",
       "    tokenUsage: { completionTokens: \u001b[33m15\u001b[39m, promptTokens: \u001b[33m11\u001b[39m, totalTokens: \u001b[33m26\u001b[39m },\n",
       "    finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const model = new ChatOpenAI();\n",
    "\n",
    "await model.invoke([\n",
    "    new HumanMessage(\"Tell me a joke\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Why did the tomato turn red? Because it saw the salad dressing!\"\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const parser = new StringOutputParser();\n",
    "const model = new ChatOpenAI();\n",
    "\n",
    "const chain = model.pipe(parser)\n",
    "\n",
    "await chain.invoke([\n",
    "    new HumanMessage(\"Tell me a joke\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ answer: \"达芬奇\", evidence: \"有历史文献证明蒙娜丽莎的作者是达芬奇\", confidence: \"95%\" }\n",
      "You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
      "\n",
      "\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
      "\n",
      "For example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\n",
      "would match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\n",
      "Thus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n",
      "\n",
      "Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match the schema exactly and there are no trailing commas!\n",
      "\n",
      "Here is the JSON Schema instance your output must adhere to. Include the enclosing markdown codeblock:\n",
      "```json\n",
      "{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"用户问题的答案\"},\"evidence\":{\"type\":\"string\",\"description\":\"你回答用户问题所依据的答案\"},\"confidence\":{\"type\":\"string\",\"description\":\"问题答案的可信度评分，格式是百分数\"}},\"required\":[\"answer\",\"evidence\",\"confidence\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import { StructuredOutputParser } from \"langchain/output_parsers\";\n",
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "const parser = StructuredOutputParser.fromNamesAndDescriptions({\n",
    "  answer: \"用户问题的答案\",\n",
    "  evidence: \"你回答用户问题所依据的答案\",\n",
    "  confidence: \"问题答案的可信度评分，格式是百分数\",\n",
    "});\n",
    "\n",
    "const prompt = PromptTemplate.fromTemplate(\"尽可能的回答用的问题 \\n{instructions} \\n{question}\")\n",
    "const model = new ChatOpenAI();\n",
    "\n",
    "const chain = prompt.pipe(model).pipe(parser)\n",
    "const res = await chain.invoke({\n",
    "    question: \"蒙娜丽莎的作者是谁？是什么时候绘制的\",\n",
    "    instructions: parser.getFormatInstructions()\n",
    "})\n",
    "                               \n",
    "console.log(res)\n",
    "\n",
    "\n",
    "console.log(parser.getFormatInstructions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n",
      "[ \"Google\", \"Amazon\", \"Facebook\" ]\n"
     ]
    }
   ],
   "source": [
    "import { CommaSeparatedListOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "const parser = new CommaSeparatedListOutputParser();\n",
    "\n",
    "console.log(parser.getFormatInstructions())\n",
    "\n",
    "const model = new ChatOpenAI();\n",
    "const prompt = PromptTemplate.fromTemplate(\"列出3个 {country} 的著名的互联网公司.\\n{instructions}\")\n",
    "    \n",
    "const chain = prompt.pipe(model).pipe(parser)\n",
    "\n",
    "const response = await chain.invoke({\n",
    "    country: \"America\",\n",
    "    instructions: parser.getFormatInstructions(),\n",
    "});\n",
    "\n",
    "console.log(response);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ answer: \"达·芬奇 (Leonardo da Vinci)\", confidence: 95 }\n",
      "{ answer: \"蒙娜丽莎的作者是达芬奇，大约在16世纪初期（1503年至1506年之间）开始绘制。\", confidence: 90 }\n"
     ]
    }
   ],
   "source": [
    "import { z } from \"zod\";\n",
    "import { StructuredOutputParser, OutputFixingParser } from \"langchain/output_parsers\";\n",
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const schema = z.object({\n",
    "  answer:  z.string().describe(\"用户问题的答案\"),\n",
    "  confidence: z.number().min(0).max(100).describe(\"问题答案的可信度评分，满分 100\")\n",
    "});\n",
    "\n",
    "const parser = StructuredOutputParser.fromZodSchema(schema);\n",
    "const prompt = PromptTemplate.fromTemplate(\"尽可能的回答用的问题 \\n{instructions} \\n{question}\")\n",
    "const model = new ChatOpenAI();\n",
    "\n",
    "const chain = prompt.pipe(model).pipe(parser)\n",
    "const res = await chain.invoke({\n",
    "    question: \"蒙娜丽莎的作者是谁？是什么时候绘制的\",\n",
    "    instructions: parser.getFormatInstructions()\n",
    "})\n",
    "                               \n",
    "console.log(res)\n",
    "\n",
    "\n",
    "\n",
    "const fixParser = OutputFixingParser.fromLLM(model, parser);\n",
    "const wrongOutput = {\n",
    "  \"answer\": \"蒙娜丽莎的作者是达芬奇，大约在16世纪初期（1503年至1506年之间）开始绘制。\",\n",
    "  \"sources\": \"90%\" \n",
    "};\n",
    "\n",
    "const output = await fixParser.parse(JSON.stringify(wrongOutput));\n",
    "console.log(output);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  Document {\n",
      "    pageContent: \"三体前传：球状闪电 作者：刘慈欣\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"内容简介：\\n\" +\n",
      "      \"　　没有《球状闪电》，就没有后来的《三体》！\\n\" +\n",
      "      \"　　《三体》前传！\\n\" +\n",
      "      \"　　亚洲首位雨果奖得主刘慈欣的三大长篇之一！（《三体》《球状闪电》《超新星纪元》）\\n\" +\n",
      "      \"　　《球状闪电》拥有最狂野的想象力！\\n\" +\n",
      "      \"　　带你从另一个维度观察世界！\\n\" +\n",
      "      \"　　全面展现刘慈欣对人生的终极思考！\\n\" +\n",
      "      \"　　《三体》中解决可控核聚变的顶级物理学家丁仪，正是《球状闪电》中找到球状闪电的关键人物。\\n\" +\n",
      "      \"　　《三体》中一号面壁人的幽灵军队计划，正是来自于《球状闪电》。\\n\" +\n",
      "      \"　　《球状闪电》中的疑似宇宙观察者，为《三体》中的智子出现铺陈了线索。\\n\" +\n",
      "      \"　　《球状闪电》和《三体》是刘慈欣两个不同时期的巅峰之作，《球状闪电》是他对人生的终极思考，《三体》则是他对宇宙的终极思考。\\n\" +\n",
      "      \"　　过一个美妙的人生并不难，关键在于你迷上的是什么。\\n\" +\n",
      "      \"　　某个离奇的雨夜，一颗球状闪电在一瞬间将少年的父母化为灰烬，而他们身下的板凳却分毫无损。\\n\" +\n",
      "      \"　　从此少年踏上了研究球状闪电的旅程，死亡者的幽灵笔迹、前苏联的地下科技城、次世代的世界大战……\\n\" +\n",
      "      \"　　球状闪电意外成为了战争中决定祖国存亡的终极武器！\\n\" +\n",
      "      \"　　一个从未有人想像过的未来，在宇宙观察者的注视下，悄然降临在人类面前……\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"说明\\n\" +\n",
      "      \"　　本书中对球状闪电特性和行为的描写均以真实历史记录为依据\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"序曲\\n\" +\n",
      "      \"　　今天是我的生日，直到晚上爸爸妈妈点上了生日蛋糕的蜡烛，我们三个围着十四个小火苗坐下来，我才想起这事。\\n\" +\n",
      "      \"　　这是个雷雨之夜，整个宇宙似乎是由密集的闪电和我们的小屋组成。当那蓝色的电光闪起时，窗外的雨珠在一瞬间看得清清楚楚，那雨珠似乎凝固了，像密密地挂在天地间的一串串晶莹的水晶。这时我的脑海中就有一个闪念：世界要是那样的也很有意思，你每天一出门，就在那水晶的密帘中走路，它们在你周围发出丁零丁零的响声，只是，这样玲珑剔透的世界，如何经得住那暴烈的雷电呢……世界在我的眼中总和在别人眼中不一样，我总是努力使世界变形，这是我长这么大对自己唯一的认识。\\n\" +\n",
      "      \"　　暴雨是从傍晚开始的，自那以后闪电和雷声越来越密，开始，每当一道闪电过后，我脑海中一边回忆着刚才窗外那转瞬即逝的水晶世界，一边绷紧头皮等待着那一声炸雷，但现在，闪电太密集了，我已分不出哪声雷属于哪个闪电了。\\n\" +\n",
      "      \"　　在这狂暴的雷雨之夜最能体会出家的珍贵，想象着外面那恐怖危险的世界，家的温暖怀抱让人陶醉。这时，你会深深同情外面大自然中那些在暴雨和雷电下发抖的没有家的生灵，你想打开窗子让它们飞进来，但你又不敢这么做，外面的世界太可怕，你不敢让一丝外面的恐怖气息进入到家的温暖的空间里来。\\n\" +\n",
      "      \"　　“人生啊，人生这东西……”爸爸一口气喝干了一大杯酒，眼睛直勾勾地看着那一簇小火苗说，“变幻莫测，一切都是概率和机遇，就像在一条小溪中漂着的一根小树枝，让一块小石头绊住了，或让一个小旋涡圈住了……”\\n\" +\n",
      "      \"　　“孩子还小，听不懂这些。”妈妈说。\\n\" +\n",
      "      \"　　“他不小了！”爸爸说，“他已到了可以知道人生真相的时候了！”\\n\" +\n",
      "      \"　　“你自己好像知道似的。”妈妈带着嘲讽的笑说。\\n\" +\n",
      "      \"　　“我知道，当然知道！”爸爸又干了半杯酒，然后转向我，“其实，儿子，过一个美妙的人生并不难，听爸爸教你：你选一个公认的世界难题，最好是只用一张纸和一支铅笔的数学难题，比如哥德巴赫猜想或费尔马大定理什么的，或连纸笔都不要的纯自然哲学难题，比如宇宙的本源之类，投入全部身心钻研，只问耕耘不问收获，不知不觉的专注中，一辈子也就过去了。人们常说的寄托，也就是这么回事。或是相反，把挣钱作为唯一的目标，所有的时间都想着怎么挣，也不用问挣来干什么用，到死的时候像葛朗台一样抱着一堆金币说：啊，真暖和啊……所以，美妙人生的关键在于你能迷上什么东西。比如我——”爸爸指指房间里到处摆放着的那些小幅水彩画，它们的技法都很传统，画得中规中矩，从中看不出什么灵气来。这些画映着窗外的电光，像一群闪动的屏幕，“我迷上了画画，虽然知道自己成不了凡·高。”\\n\" +\n",
      "      \"　　“是啊，理想主义者和玩世不恭的人都觉得对方很可怜，可他们实际都很幸运。”妈妈若有所思地说。\\n\" +\n",
      "      \"　　平时成天忙碌的爸爸妈妈这时都变成了哲学家，倒好像这是他们在过生日。\\n\" +\n",
      "      \"　　“妈，别动！”我说着，从妈妈看上去乌黑浓密的头发中拔出一根白头发，只白了一半，另一半还是黑的。\\n\" +\n",
      "      \"　　爸爸拿着那根头发对着灯看了看，闪电中，它像灯丝似的发出光来。“据我所知，这是你妈妈有生以来长出的第一根白发，至少是第一次发现。”\\n\" +\n",
      "      \"　　“干什么吗你？！拔一根要长七根的！”妈妈把头发甩开，恼怒地说。\\n\" +\n",
      "      \"　　“唉，这就是人生了。”爸爸说，他指着蛋糕上的蜡烛，“想想你拿着这么一根小蜡烛，放到戈壁滩上去点燃它，也许当时没风，真让你点着了，然后你离开，远远地你看着那火苗有什么感觉？孩子，这就是生命和人生，脆弱而飘忽不定，经不起一丝微风。”\\n\" +\n",
      "      \"　　我们三个都默默无语地看着那一簇小火苗，看着它们在从窗外射入的冰冷的青色电光中颤抖，像是看着我们精心培育的一窝小生命。\\n\" +\n",
      "      \"　　窗外又一阵剧烈闪电。\\n\" +\n",
      "      \"　　这时它来了，是穿墙进来的，它从墙上那幅希腊众神狂欢的油画旁出现，仿佛是来自画中的一个幽灵。它有篮球大小，发着朦胧的红光。它在我们的头顶上轻盈地飘动着，身后拖着一条发出暗红色光芒的尾迹，它的飞行路线变幻不定，那尾迹在我们上方划出了一条令人迷惑的复杂曲线。它在飘动时发出一种啸叫，那啸叫低沉中透着尖利，让人想到在太古的荒原上，一个鬼魂在吹着埙。\\n\" +\n",
      "      \"　　妈妈惊恐地用双手抓住爸爸，我恨她这个动作恨了一辈子，如果她没那样做，我以后可能至少还有一个亲人。\\n\" +\n",
      "      \"　　它继续飘着，仿佛在寻找着什么，终于它找到了。它悬停在爸爸头顶上半米处，啸叫声变得低沉，断断续续，仿佛是冷笑。\\n\" +\n",
      "      \"　　这时我可以看到它的内部，那半透明的红色辉光似乎有无限深，从那不见底的光雾的深渊中，不断地有大群蓝色的小星星飞出来，像是太空中一个以超光速飞行的灵魂所看到的星空。\\n\" +\n",
      "      \"　　后来知道，它的内部能量密度高达每立方厘米两万至三万焦耳，而即使是TNT炸药的能量密度也不过每立方厘米两千焦耳。虽然它的内部温度高达一万多度，表面却是冷凉的。\\n\" +\n",
      "      \"　　爸爸向上伸出手，他显然并不是去摸它，而是想护住自己的头部。当他的手伸到最高点时，似乎产生了一种吸力，把它吸到手上，就像一片叶子的细尖吸下了一滴露珠。\\n\" +\n",
      "      \"　　一道炫目的白炽，一声巨响，仿佛世界在身边爆炸。\\n\" +\n",
      "      \"　　当眼睛因强光造成的暗雾散去后，我看到了将伴随我一生的景象：像在图像处理软件的色彩模式中选了黑白一样，爸爸和妈妈的身体瞬间变成了黑白两色的，更确切地说是灰白色，黑色是灯光在皱折处照出的阴影。那是一种大理石的颜色。爸爸的手仍旧向上举着，妈妈仍旧倾身用双手抓着爸爸的另一只手臂，在这两尊雕像的面容上，那两双已石化的眼睛仍旧栩栩如生。\\n\" +\n",
      "      \"　　空气中有一种怪异的气味，后来我知道那是臭氧的气味。\\n\" +\n",
      "      \"　　“爸！”我喊了一声。没有回答。\\n\" +\n",
      "      \"　　“妈！”我又喊了一声。没有回答。\\n\" +\n",
      "      \"　　我向那两尊雕像靠过去，这是我一生中最恐惧的时刻。我以前经历过的恐惧大多在梦中，在噩梦的世界中我之所以没有精神崩溃，是因为我的一个下意识在梦中仍醒着，一个声音在我意识最偏远的角落对我喊：这是梦。我现在也在心里拼命地冲自己这样喊，这是支撑我走过去的唯一动力。我伸出颤抖的手，去触碰爸爸的身体，当我的手接触到他肩部那灰白色的表面时，感觉像是穿透了一层极薄极脆的薄壳。我听到了轻微的噼啪声，像是严冬时倒入开水的玻璃杯的爆裂声，两尊雕像在我眼前坍塌下去，像一场微型的雪崩。\\n\" +\n",
      "      \"　　地毯上出现了两堆白灰，除此之外什么都没有了。\\n\" +\n",
      "      \"　　但他们坐过的木凳还在那里，上面也落了一层灰。我拂去上面的灰，看到它的表面完好无损，而且摸上去是冰凉凉的。我知道，在火葬场的炉子中，要把人体完全化为灰烬，要在两千度的高温下烧三十分钟，所以这是梦。\\n\" +\n",
      "      \"　　我茫然四顾，看到有烟从书架中冒出来，有玻璃门的书架中充满了白烟。我走过去拉开书架的门，白烟散尽，我看到里面的书约有三分之一变成灰烬，颜色同地毯上那两堆灰一样，但书架没有任何烧过的痕迹，这是梦。\\n\" +\n",
      "      \"　　我看到一股蒸汽从半开的冰箱中冒出，走过去拉开冰箱门，发现里面的一只生冻鸡已变成熟的，发出一股香味，还有那些生对虾和生鱼，都熟了，但冰箱完好无损，正发出压缩机启动时的声响，这是梦。\\n\" +\n",
      "      \"　　我身上有些异样的感觉，拉开夹克，一片灰烬从我的身上散落下来，我里面穿的背心被烧成了灰，外面的夹克好好的，我刚才更没感觉到什么。我翻夹克的口袋，手被狠狠烫了一下，拿出来一看，装在里面的掌上机已变成一团熔化塑料。这的确是梦，好奇妙的梦啊！\\n\" +\n",
      "      \"　　我木然地坐回我的位子上，我看不到桌子对面地毯上那两小堆灰，但知道它们在那儿。外面的雷声弱了，闪电少了，后来雨停了，再后来月亮从云缝中探出来，把一抹神秘的银光投进窗。我仍木然地坐在那儿，一动不动，这时在我的意识中世界已不存在，我悬浮在无际的虚空中。不知过了多长时间，窗外的朝阳唤醒了我，我木然地站起身，拿起书包去上学，我要摸索着找书包，摸索着打开门，因为我的两眼一直木然地看着无限远方……\\n\" +\n",
      "      \"　　当一个星期后我的精神基本恢复正常时，记起来的第一件事就是那夜是我的生日之夜，但那个蛋糕上应该只插一根蜡烛，哦不，一根都不插，那是我的新生之夜，以后的我再也不是以前那个我了。\\n\" +\n",
      "      \"　　像爸爸在生命的最后时刻说的那样，我迷上了一样东西，我要去经历他所说的美妙人生了。\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"上篇\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"大学\\n\" +\n",
      "      \"　　主要课程：高等数学、理论力学、流体力学、计算机原理及应用、计算机语言及程序设计、动力气象、天气学原理、中国天气、统计预报、中长期天气预报、数值预报等；\\n\" +\n",
      "      \"　　选修课有：大气环流、天气学诊断分析、暴雨与中尺度天气、雷暴预测及避防、热带天气、气候变化与短期气候预测、雷达气象和卫星气象、空气污染与城市气候、高原天气、大气海洋相互作用等。\\n\" +\n",
      "      \"　　五天前，我处理了家里的所有东西，到这座千里之外的南方城市来上大学。当我最后一次关上已经空荡荡的家门时，知道自己把童年和青春永远留在那里了，以后的我，将是单纯追寻一个目标的机器。\\n\" +\n",
      "      \"　　看着这份将占据我四年大学生活的课程清单，我多少有些失望。里面大多数的东西是我不需要的，而有些我最需要的东西，比如电磁学和等离子体物理之类的课程，又没有。我知道自己可能报错了专业，应该报物理专业而不是大气科学专业。\\n\" +\n",
      "      \"　　以后，我一头扎进了图书馆，把几乎所有的时间都花在数学、电磁学、流体力学和等离子体物理上，只有当有涉及这些内容的课时我才去听，其他的课一般都不去。丰富多彩的大学生活与我无关，我也不感兴趣。我每天夜里都在一两点才回到宿舍，听着某个室友在梦中喃喃地念着女朋友的名字，这才意识到还有另一种生活。\\n\" +\n",
      "      \"　　有一天晚上，十二点已过，我从那本厚厚的《偏微分方程》上抬起头来，以为这间专为夜读的学生开的阅览室中又是只剩我一人了，但看到桌对面坐着一个本班叫戴琳的漂亮女生，她面前没有书，只是用双手撑着脑袋看着我。即使对她的那一大堆追求者来说，这目光也不会让他们陶醉，那是一种在己方阵营中发现间谍的目光，一种看异类的目光，我不知道她已这样看了我多长时间。\\n\" +\n",
      "      \"　　“你这人很特别，看得出来，你不是书呆子，你的目的性很强。”她说。\\n\" +\n",
      "      \"　　“嗯？你们没有目的吗？”我随口问，也许，我是在班上唯一没同她说过话的男生。\\n\" +\n",
      "      \"　　“我们的目的是泛泛的，而你，你肯定在找什么很具体的东西！”\\n\" +\n",
      "      \"　　“你看人很准。”我冷冷地说，同时收拾书站起身。我是唯一不需时时对她表现自己的人，所以有一种优越感。\\n\" +\n",
      "      \"　　“你在找什么？”当我走到门口时，她在后面喊。\\n\" +\n",
      "      \"　　“你不会感兴趣的。”我头也不回地走了。\\n\" +\n",
      "      \"　　在外面宁静的秋夜中，我看着满天繁星，空中似乎传来了爸爸的声音：“美妙人生的关键在于你能迷上什么东西。”我现在真正体会到他这话的正确，我现在的人生好比一颗疾飞的炮弹，除了对到达目标时那一声爆炸的渴望之外什么都没有。这个目标完全是非功利的，达到它就意味着生活的完结，我不知道为什么要去那儿，我只是想去，这就够了，这是人类最本源的冲动。很奇怪的，到现在为止，我一次都没有去查过它的资料。我和它，像两个要用一生时间准备一场决斗的骑士，当我没准备好的时候，既不去见它也不去想它。\\n\" +\n",
      "      \"　　转眼三个学期过去了，这段时间在我的感觉中很连续，并没有被假期打断，无家可归的我所有的假期都在学校里度过。一个人住在空旷的宿舍楼中，我丝毫没有孤独感，只有在除夕之夜，听着外面的鞭炮声，我才多少想到了它出现之前的生活，那生活已恍若隔世。这几夜，在停了暖气的宿舍中，寒冷使我的梦格外生动，我本以为这一夜爸爸妈妈会在梦中出现，但他们没有来。记得有一个印度传说，说一个国王所深爱的王妃死去，国王决定为她建造一座前所未有的豪华陵墓，他为这座陵墓耗尽了大半生的心血，当陵墓完工时，他看到正中放着的王妃的棺木，说：这东西放在这儿多不协调，把它搬走。\\n\" +\n",
      "      \"　　在我的心中，爸爸妈妈已远去了，现在占据了全部位置的是它。\\n\" +\n",
      "      \"　　但接下来的事情，使我自己那本已很简单的世界又复杂起来。\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"异象之一\\n\" +\n",
      "      \"　　大二的暑假，我回了一趟家，是为了把那套旧房子租出去，以解决我以后的学杂费。\\n\" +\n",
      "      \"　　回到家时天已经黑了，我摸索着开了锁推门进去，开灯后看到了那熟悉的一切。那张曾在那个雷雨之夜放过生日蛋糕的桌子仍摆在屋正中，那三把椅子也仍在桌边放着，仿佛我昨天才离开。我在沙发上疲惫地坐下，打量着自己的家，感觉有什么地方不对，这种感觉开始很模糊，后来却越来越明显，好像迷雾的航程中时隐时现的暗礁，让我不得不正视它，终于，我找到了这感觉的源泉：\\n\" +\n",
      "      \"　　仿佛昨天才离开。\\n\" +\n",
      "      \"　　我仔细看看桌面，上面有一层薄薄的灰尘，但相对于我离去的这两年时间，这灰尘确实太薄了些。\\n\" +\n",
      "      \"　　我一脸的汗水和尘土，就走进卫生间去洗脸。打开灯后，看到了镜子中清晰的自己，是的，太清晰了，镜子不应该这么干净的。清楚地记得小学时的一个暑假，我和父母一起外出旅游，只走了一个星期，回来后我就用手指在镜面的灰尘上画出一个小人儿来，现在我又用手指在镜面上画了几下，什么都没画出来。\\n\" +\n",
      "      \"　　我拧开水龙头，关了两年的铁管龙头，流出的应是充满铁锈的浑水，但现在流出的水十分清亮。\\n\" +\n",
      "      \"　　洗完脸回到客厅，我又注意到了另外一件事：两年前我最后离开时，关门前匆匆看了屋里一眼，怕忘了什么，看到桌上放着我的一个玻璃杯，就想回去把杯子倒扣过来以免落进灰尘，但肩上背着行李包，再进门有些费劲，就打消了这个念头，这个细节我记得很清楚。\\n\" +\n",
      "      \"　　但现在，桌上的那个杯子是倒扣着的！\\n\" +\n",
      "      \"　　这时，邻居们看到灯光走了进来，都向我说起对一名上大学的孤儿该说的亲切温暖的话，并许诺为我代办房屋出租的事宜，如果将来毕业后不能回来，还负责为我将这套房卖个好价钱。\\n\" +\n",
      "      \"　　“这里的环境好像比我走时干净了许多。”谈到这两年的变化时，我随口说了一句。\\n\" +\n",
      "      \"　　“干净了？你什么眼神啊！靠酒厂那边的那个火电厂在去年投产发电了，现在的烟尘比你走时多了一倍！嘿，现在还有能变干净的地方？”\\n\" +\n",
      "      \"　　我看看那只有薄薄灰尘的桌面，没说什么，但当他们告辞时，还是忍不住问了一句他们中是否谁有我家的家门钥匙。邻居们惊奇地互相看看，都肯定地说没有，我相信他们，因为家门共有五把钥匙，现在完好的还剩三把，我两年前离开时都带走了，有一把现在我带着，另外两把留在我远方的大学宿舍中。\\n\" +\n",
      "      \"　　邻居们走后我又检查了所有的窗户，都牢牢地关着，没有被破坏的痕迹。\\n\" +\n",
      "      \"　　还有另外两把家门钥匙，是我父母带着的。但是，在那个夜里，它们都被熔化了。我不可能忘记自己是怎样从父母的骨灰堆中找出那两块形状不规则的金属，那是熔化后又凝结的两串钥匙，它们现在也放在我那千里之外的宿舍中，作为对那种不可思议的能量的纪念。\\n\" +\n",
      "      \"　　我坐了一会儿，开始收拾东西，这些东西是在房间出租后准备寄存到别处或带走的。我首先收拾的是父亲的那些水彩画，它们是这个房间里为数不多的我真正想保留的东西。我首先把墙上挂着的那几幅取下来，接着取出放在柜子中的，我尽可能地把所有的画都找出来，把它们一起装进纸箱。最后看到书架的底层还有一幅，由于它画面朝下放着，所以刚才没注意到。把这幅画放进箱子前我瞟了一眼画面，目光立刻被钉死在上面。\\n\" +\n",
      "      \"　　这是一幅风景画，画的是在我家门口看到的景物。这周围的景色平淡乏味，几幢灰暗的四层旧楼房，几排白杨，因落满灰尘而显得没什么生气……作为一名三流业余画家的父亲是很懒的，他很少外出写生，只是乐此不疲地画着周围这些灰蒙蒙的景色，还说什么没有平淡的景色，只有平庸的画家。而他就是一个这样的画家，这些平淡的景色经过他那没有灵气的画笔的临摹，更添了一层呆板，倒真是这灰暗的北方城市日常生活的写照。我现在手里拿着的就是这样一幅画，与箱子里许多张类似的画一样，没什么特别引人之处。\\n\" +\n",
      "      \"　　但我注意到画中有一样东西，那是一座水塔，与周围的旧楼相比它的色彩稍微艳丽了一些，像一朵高大的喇叭花。这本来也没有什么特别之处，外面，那座水塔确实存在，我抬头看看窗外，看到它那高高的塔身在城市的灯光前呈一个漆黑的剪影。\\n\" +\n",
      "      \"　　只是，这座水塔是在我考上大学之后才建成的，我两年前离开时，塔身只在脚手架中建了一半。\\n\" +\n",
      "      \"　　我浑身颤抖了一下，手中的画掉在地上。在这盛夏之夜，似乎有一股寒气充满了这个家。\\n\" +\n",
      "      \"　　我把那幅画塞进纸箱，把箱子严严地盖好，转身去收拾其他东西。我努力把注意力集中在正在干的事上，但我的思想仿佛是一根用细丝悬吊着的铁针，而那个纸箱子是一块强磁铁，我可以努力将针转向其他方向，但只要这种努力一松懈，针立刻又被吸回那个方向。外面下雨了，雨滴打在窗玻璃上发出轻响，我总觉得这响声是从那个箱子中发出的……最后，实在忍受不了，我快步走向纸箱，将它打开来，把那幅画拿出来，小心地将画面朝下拿着它走向卫生间，掏出打火机从一角点燃了它。当画烧到三分之一时，我忍不住又将它翻了过来，画面上的那座水塔更加栩栩如生，仿佛要从画纸上凸现出来。我看着火焰吞没了它，画出它的水彩被烧焦了，火苗呈现一种怪异而妖艳的色彩。我把将要烧尽的画扔进盥洗池，看着它烧完，然后打开水龙头，将灰烬冲走。关上水龙头后，我的目光落到了盥洗池的池沿上，看到了刚才洗脸时没注意的东西。\\n\" +\n",
      "      \"　　几根头发，很长的头发。\\n\" +\n",
      "      \"　　那是几根白发，有的全白，与池面几乎融为一体；有的则白了一半，正是那些黑的部分使我看到了它们。这不可能是我两年前留下的，我从来没有过这么长的头发，更没有白发。我轻轻拿起其中一根半黑半白的长发。\\n\" +\n",
      "      \"　　……拔一根长七根……\\n\" +\n",
      "      \"　　我将头发扔掉，仿佛它烫手似的。那根头发在空气中慢慢飘落，竟拖着一道尾迹，那尾迹是由许多头发自身的转瞬即逝的映像组成，就好像我的视觉暂留时间延长了许多似的。这根头发并没有落回池沿上，它只落了一半的高度就在半空中消失了。我再看池沿上其他头发，它们也都消失得无影无踪了。\\n\" +\n",
      "      \"　　我把脑袋放到水龙头下冲了好长时间，然后木然地回到客厅，坐在沙发上，听着外面的雨声。雨已经下得很大了，是一场暴雨，但没有雷声和闪电。雨打在窗上，听上去像一个人或许多人的低语，仿佛在提醒我什么。听久了，我渐渐想象出了那低语的内容，它一遍遍地重复着，听起来越来越真实：\\n\" +\n",
      "      \"　　“那天有雷，那天有雷，那天有雷，那天有雷，那天有雷……”\\n\" +\n",
      "      \"　　我再次在一个暴雨之夜在家里一直坐到天亮，然后再次木然地离开了家，我知道自己把什么东西永远留在这里，也知道自己永远不会再回来了。\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"球状闪电\\n\" +\n",
      "      \"　　我必须要面对它了，因为开学后，大气电学专业的课程就要开始了。\\n\" +\n",
      "      \"　　讲大气电学的是一名叫张彬的副教授，这人五十岁左右，个子不高不矮，眼镜不薄不厚，讲话声音不高不低，课讲得不好不坏，总之，是那种最一般的人，他唯一与众不同的地方是腿有些瘸，但不注意就看不出来。\\n\" +\n",
      "      \"　　这天下午下课后，阶梯教室中只剩我和张彬两人，他在讲台上收拾东西，没有注意到我。时值深秋，夕阳把几缕金色的光投进来，窗台上落了一层金黄色的落叶，内心一向冷漠的我突然意识到，这是作诗的季节了。\\n\" +\n",
      "      \"　　我站起来走到讲台前，“张老师，我想请教个问题，与今天的课无关。”\\n\" +\n",
      "      \"　　张彬抬头看了我一眼，点了点头，又低头收拾东西。\\n\" +\n",
      "      \"　　“关于球状闪电，您能告诉我些什么？”我说出了那个一直深埋在心中但从未说出口的的词。\\n\" +\n",
      "      \"　　张彬的手停止了动作，抬起头，但没看我，而是看着窗外的夕阳，仿佛那就是我指的东西。“你想知道些什么？”过了几秒钟他才问。\\n\" +\n",
      "      \"　　“关于它的一切。”我说。\\n\" +\n",
      "      \"　　张彬一动不动地直视着夕阳，任阳光直射到脸上，这时阳光仍然很亮，他就不觉得刺眼吗？\\n\" +\n",
      "      \"　　“比如，它的历史记录。”我不得不问得更详细些。\\n\" +\n",
      "      \"　　“在欧洲，它在中世纪就有记载；在中国，比较详细的记载是明代的张居正写下的。但直到1837年才有了第一次正规的科学记载，作为一种自然现象，它在最近四十年才为科学界所接受。”\\n\" +\n",
      "      \"　　“那么，关于它的理论呢？”\\n\" +\n",
      "      \"　　“有很多种。”张彬简单地说了一句后又不吱声了。他把目光从夕阳上收回来，但没有接着收拾东西，像在深思着什么。\\n\" +\n",
      "      \"　　“最传统的理论是什么？”\\n\" +\n",
      "      \"　　“认为它是一种涡旋状高温等离子体，由于内部高速旋转造成的离心力与外部大气压力达到平衡，因而维持了较长时间的稳定性。”\\n\" +\n",
      "      \"　　“还有吗？”\\n\" +\n",
      "      \"　　“还有人认为它是高温混合气体之间的化学反应，从而维持了能量的稳定。”\\n\" +\n",
      "      \"　　“您能告诉我更多一些吗？”我说。向他提问，如同费力地推着一个沉重的石碾子，推一下才动一下。\\n\" +\n",
      "      \"　　“还有微波激射—孤立子理论，认为球状闪电是由体积约为若干立方米的大气微波激射所引起的。微波激射相当于能量低得多的激光，在空气体积很大时，微波激射会产生局部电场即孤立子，从而导致看得见的球状闪电。”\\n\" +\n",
      "      \"　　“那么最新的理论呢？”\\n\" +\n",
      "      \"　　“也有很多，比较受到注意的是新西兰坎特伯雷大学的亚伯拉罕森和迪尼斯的理论，认为球状闪电主要是由微型含硅颗粒组成的网络球体燃烧形成。其他的五花八门，甚至有人认为它是空气中的常温核聚变。”\\n\" +\n",
      "      \"　　张彬停了一下，终于说出了更多的内容：“在国内，中科院大气所有人提出了大气中等离子体的理论，从电磁流体力学方程出发，引入旋涡—孤立子谐振腔模型，在适当温度场边界条件下，通过数值求解方程，从理论上得到了大气中等离子体涡团—火球的解以及它存在的必要和充分条件。”\\n\" +\n",
      "      \"　　“您认为这些理论怎么样？”\\n\" +\n",
      "      \"　　张彬缓缓地摇了摇头，“要证明这些理论的正确，只有在实验室中产生出球状闪电，但至今没人能成功。”\\n\" +\n",
      "      \"　　“在国内，目击球状闪电的案例有多少？”\\n\" +\n",
      "      \"　　“不少，有上千份吧。其中最著名的是1998年中央电视台拍摄的长江抗洪纪录片中，无意间清晰地摄下了一个球状闪电。”\\n\" +\n",
      "      \"　　“张老师，最后一个问题：在国内大气物理学界，有亲眼看见过它的人吗？”\\n\" +\n",
      "      \"　　张彬又抬头看窗外的夕阳：“有。”\\n\" +\n",
      "      \"　　“什么时间？”\\n\" +\n",
      "      \"　　“1962年7月。”\\n\" +\n",
      "      \"　　“什么地方？”\\n\" +\n",
      "      \"　　“泰山玉皇顶。”\\n\" +\n",
      "      \"　　“您知道这人现在在哪儿吗？”\\n\" +\n",
      "      \"　　张彬摇了摇头，抬腕看了看表，“你该去食堂打饭了。”说完拿起他的东西径自朝外走去。\\n\" +\n",
      "      \"　　我追上了他，把这么多年来自己心中的问题全部倾泻出来，“张老师，您能够想象有这么一种东西，以一团火球的形式毫不困难地穿过墙壁，在空气中飞行时你感觉不到它的一点热量，却能瞬间把人烧成灰？有记载它曾把睡在被窝里的一对夫妻烧成灰，被子上却连一道焦痕都没留下！您能想象它进入冰箱，瞬间使里面的所有冷冻食品都变成冒热气的熟食，而冰箱本身还在不受任何影响地运转？您能想象它把您的贴身衬衣烧焦，而您竟没有感觉？您说的那些理论能解释这一切吗？”\\n\" +\n",
      "      \"　　“我说过那些理论都不成立。”张彬说，他没有停步。\\n\" +\n",
      "      \"　　“那么，我们越出大气物理学的范围，您认为现今的整个物理学，甚至整个科学能解释这现象吗？您就丝毫不感到好奇？看到您这样，我真比见到球状闪电还吃惊！”\\n\" +\n",
      "      \"　　张彬停下了脚步，转过身来第一次正视我，“你见过球状闪电？”\\n\" +\n",
      "      \"　　“……我只是比喻。”\\n\" +\n",
      "      \"　　我无法把内心最深处的秘密告诉眼前这个麻木的人，这种对大自然那深邃神秘的麻木充斥着整个社会，对科学来说早就是一种公害。如果这种人在学术界少一些，人类现在说不定已飞抵人马座了！\\n\" +\n",
      "      \"　　张彬说：“大气物理学是一门很实用的科学，球状闪电是一种极其罕见的现象，在国际建筑物防雷标准IEC/TC-81，以及我国1993年颁布的《建筑物防雷设计规范》中，都没有考虑到它，所以，在这东西上花太多的精力，意义不大。”\\n\" +\n",
      "      \"　　和这种人真没什么太多的话好讲，我谢过他转身走人。要知道，他能承认球状闪电的存在，已经是一大进步了！直到1963年，科学界才正式认同这种闪电的存在，这之前，所有的目击报告都被断定为幻觉。这一年的一天，美国肯特大学\"... 179494 more characters,\n",
      "    metadata: { source: \"data/qiu.txt\" }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { TextLoader } from \"langchain/document_loaders/fs/text\";\n",
    "const loader = new TextLoader(\"data/qiu.txt\");\n",
    "\n",
    "const docs = await loader.load();\n",
    "\n",
    "console.log(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  Document {\n",
      "    pageContent: \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot1/14\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常\\n\" +\n",
      "      \"工作\\n\" +\n",
      "      \"Nov 25, 2023\\n\" +\n",
      "      \"LLMchinese\\n\" +\n",
      "      \"“原文来自于 x 推文，所以保留了原始分段的⻛格”\\n\" +\n",
      "      \"0. 一些基础信息\\n\" +\n",
      "      \"\\x00. github copilot 是 gpt3 针对代码场景优化而来的 Codex 模型，其基础性能不如\\n\" +\n",
      "      \"gpt4，但在代码场景效果更好\\n\" +\n",
      "      \" Kai\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot2/14\\n\" +\n",
      "      \"\\x00. copilot 不是银弹，并不是一秒解决 50% 的工作，而是将 50% 的工作时间替换\\n\" +\n",
      "      \"成了 10% 的 prompt/chat 时间\\n\" +\n",
      "      \"\\x00. 认清 copilot 的定位，其是一个副驾驶的⻆色，自己的思维方式要从“如何去做这\\n\" +\n",
      "      \"件事” => “如何激发 copilot 去做这件事”\\n\" +\n",
      "      \"\\x00. 尝试 ai-native 的开发方式，从自己编码 + ai copilot 到自己编写 prompt、\\n\" +\n",
      "      \"copilot 编码，然后自己去进行修改\\n\" +\n",
      "      \"\\x00. copilot 已经非常强，但还是一个发布并不久的工具，深度使用需要思考如何更贴\\n\" +\n",
      "      \"近它的思维和使用方式，也会遇到很多 bug\\n\" +\n",
      "      \"\\x00. 再强调一下，copilot 不是银弹，不是你告诉他需求他就能够输出完美方案的\\n\" +\n",
      "      \"bot，你只是把编码时间换成了 更少 prompt 时间\\n\" +\n",
      "      \"\\x00. 不要编程这件事妄自菲薄，不要高看也不要低看，一个学习过所有开源代码的\\n\" +\n",
      "      \"llm 编程能力是很强的。但依旧需要人类去“激活”和引导，且人类也有其独特的\\n\" +\n",
      "      \"优势\\n\" +\n",
      "      \"1. 基本使用思路\\n\" +\n",
      "      \"\\x00. 把自己的视野拉高，让 copilot 去做更低维度的事情\\n\" +\n",
      "      \"\\x00. copilot 是极度廉价劳动力，是可以让他去帮你试错、可以多开浪费他的思考来节\\n\" +\n",
      "      \"约自己的思考时间\\n\" +\n",
      "      \"\\x00. 问 copilot 的问题，自己需要至少有鉴别基础质量的能力，从而能够对他的输出\\n\" +\n",
      "      \"取其精华。在不擅⻓的领域完全信赖会导致非常严重的问题\\n\" +\n",
      "      \"\\x00. 不要懒得写⻓的 prompt，从 llm 的原理来说，你给的 context 越多，他越容易召\\n\" +\n",
      "      \"回到你想要的知识，并给你需要的答案，把他看作一个知识丰富的人类助手，用\\n\" +\n",
      "      \"给人类讲话的耐心去写 prompt。你会发现这事并不会花你太多时间\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot3/14\\n\" +\n",
      "      \"2.变量命名\\n\" +\n",
      "      \"这是非常基础但是很多人浪费了很多时间的点。你可以把你想要的这个 变量/类 想要\\n\" +\n",
      "      \"承担的任务和一些想法给到 copilot chat，然他输出你需要的命名\\n\" +\n",
      "      \"并且，copilot 的劳动力极度廉价，灵活应用 “给我十个，再给我十个，再给我十个”\\n\" +\n",
      "      \"人类想出十个合适答案的能力不如 llm，但很擅⻓从十个答案中选出合适的一个\\n\" +\n",
      "      \"3. 代码速读，代码精读，加注释解析，寻找修改项\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot4/14\\n\" +\n",
      "      \"接收其他人项目、读开源项目等情况，找到需要读的文件，全选，然后打开 copilot\\n\" +\n",
      "      \"chat（它会读取你选中的代码），使用内置的 /explian 命令，这个会内置一些 prompt\\n\" +\n",
      "      \"让输出质量更好\\n\" +\n",
      "      \"我常用的几句话是\\n\" +\n",
      "      \"“从架构设计⻆度，分析这段代码的设计思路，并讲解这种思路的优劣”\\n\" +\n",
      "      \"“分析 xxx 函数的详细逻辑，以及在整个文件中起到的作用”\\n\" +\n",
      "      \"“给 xxx 函数每一行加上注释，以详细解析该函数”\\n\" +\n",
      "      \"“我现在需要通过修改这个文件以实现 xxx 功能，如何修改？”\\n\" +\n",
      "      \"“我现在需要用 ts 重写这段 python 代码，详细解析这段 python 代码的设计逻辑，并\\n\" +\n",
      "      \"分析如何在 ts 中实现”\\n\" +\n",
      "      \"“解析这段代码中可能有哪些⻛险”\\n\" +\n",
      "      \"“在这段代码中， run 和 test 方法有什么区别”\\n\" +\n",
      "      \"copilot 的劳动力极度廉价！\\n\" +\n",
      "      \"所以在我修一个大系统的 bug 时，我会对多个可能的文件问类似于 “我的需求是\\n\" +\n",
      "      \"xxx，能通过修改这个文件实现么？”，直到找到我需要修改的地方和方案。\\n\" +\n",
      "      \"llm 读懂代码逻辑的速度极快，可以快速给你一个 80 分的答案，你再判断是否有必要\\n\" +\n",
      "      \"精读。然后再使用 copilot 辅助精读。\\n\" +\n",
      "      \"4. 代码改写，用 xx 库实现整体逻辑\\n\" +\n",
      "      \"在要用 b 库改写使用 a 库实现的逻辑时，copilot 做的非常快，因为你 a 库写的逻辑\\n\" +\n",
      "      \"就是最完美的 prompt，在实现完往往只需要通读一边确认答案即可。\\n\" +\n",
      "      \"这里涉及到对 context 的应用，而因为 codex 的数据库更新并不及时，可能并不了解\\n\" +\n",
      "      \"b 库。 那一个常用的小技巧：\\n\" +\n",
      "      \"“这是 b 库这个函数的文档，帮我改写这部分用 a 库写的逻辑”\\n\" +\n",
      "      \"“这是 b 库的官方实例，我想用 b 实现 xx 功能，帮我实现”\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot5/14\\n\" +\n",
      "      \"这种 few shot 的 prompt 技巧，可以极大程度提高输出质量。不只是在这种场景，很\\n\" +\n",
      "      \"多场景可以应用\\n\" +\n",
      "      \"5. ai-native 的开发方式\\n\" +\n",
      "      \"copilot 依旧是个初期产品，但随着发展一定会越来越强大。所以我们应该尝试使用\\n\" +\n",
      "      \"ai-native 的开发模式，学着更深入的使用 copilot.\\n\" +\n",
      "      \"我常用的技巧\\n\" +\n",
      "      \"“我需要一个 ts 类，他的使用方式和调用方式是：<伪代码>，帮我实现一个最基础的\\n\" +\n",
      "      \"版本”\\n\" +\n",
      "      \"这个其实替代了之前 模板插件 的功能，帮你更快的搭起一个 class 的基础框架，然\\n\" +\n",
      "      \"后自己填充细节。 （不会只有我每次都忘记一些 class 的语法还需要每次搜索文档\\n\" +\n",
      "      \"）\\n\" +\n",
      "      \"全选所有类代码，然后 “我给这个类添加一个 xxx 函数，帮我参考现有代码，进行实\\n\" +\n",
      "      \"现”\\n\" +\n",
      "      \"往往质量够用，甚至可以直接使用\\n\" +\n",
      "      \"“在这个 class 内，我想记录一个逐步产生的 xxx 数据，应该用什么结构比较符合 ts\\n\" +\n",
      "      \"的编程模式，帮我设计解释你的思路”\\n\" +\n",
      "      \"“这是我设计的 class/架构/数据结构，目的是 xxx，从优点和缺点各提五点理由，并详\\n\" +\n",
      "      \"细解释原因”\\n\" +\n",
      "      \"大模型的劳动力极度廉价！\\n\" +\n",
      "      \"所以先让 copilot 替你思考，很多时候他给的架构非常优秀。即使给的质量比较差，\\n\" +\n",
      "      \"一个错误的答案对你的思考也是有益的。 更何况廉价的劳动力，你可以引导他生成非\\n\" +\n",
      "      \"常多，也可以质疑他的架构，并提出你看到的问题，多次沟通直到生成有意义的架构\\n\" +\n",
      "      \"或者理清楚自己的思路。\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot6/14\\n\" +\n",
      "      \"ai-native 不是让 ai 设计架构，而是与 ai 多次讨论，让自己的思路更加清晰。\\n\" +\n",
      "      \"有时候我们知道这个架构有点问题，但不知道怎么改，ai 会给你思路。有时候我们不\\n\" +\n",
      "      \"知道这个架构有什么问题，ai 可以帮你找到问题。\\n\" +\n",
      "      \"总是，大模型的劳动力极度廉价，用他大量的思考来节约自己的思考\\n\" +\n",
      "      \"6. 报错解析\\n\" +\n",
      "      \"这是我高强度使用的一个点，首先代码报错信息是给人类读的，但又不是人类可读的\\n\" +\n",
      "      \"，且人类很难有 llm 那样无限的上下文和知识\\n\" +\n",
      "      \"除了非常基础的报错信息，先复制给 copilot chat，使用内置的 /explain 命令，让他\\n\" +\n",
      "      \"分析报错。如果是 vsc 用户，现在已经有一键操作了\\n\" +\n",
      "      \"再强调一遍，llm 不是银弹，他的答案有偏差，一定注意引导。并且，你问的问题一\\n\" +\n",
      "      \"定是你能够判断基础对错的问题。\\n\" +\n",
      "      \"常用的几句话\\n\" +\n",
      "      \"“解释这个报错，并分析可能的原因和修改方式”\\n\" +\n",
      "      \"“我认为这不是报错的根源，根据你的知识，给出三种可能的出错根源”\\n\" +\n",
      "      \"尝试一次，你就会发现，与其自己花时间去思考和分析报错，不让先让 llm 给你一个\\n\" +\n",
      "      \"80 分的答案，在大多数时间他的答案已经可以帮你解决问题了\\n\" +\n",
      "      \"7. 解释 review message\\n\" +\n",
      "      \"无论是作为一个 junior sde 还是一个开源新人，外加人类语言表达的局限性。 很多\\n\" +\n",
      "      \"review message 并没有那么明确，与其自己想半天，不如先让 llm 分析下。\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot7/14\\n\" +\n",
      "      \"复制对应的 diff 和你认为合适的上下文，附加上 review message\\n\" +\n",
      "      \"“这是我的前辈对我的 pr 的 comments，帮我分析意思，并提出合适的解决方案”\\n\" +\n",
      "      \"llm 的知识库对此做出的解析，以及对 review 黑话/缩写 的分析，往往结果还不错\\n\" +\n",
      "      \"8. 提高代码质量，设计优化\\n\" +\n",
      "      \"llm 读过的代码太多了，常用的几句话\\n\" +\n",
      "      \"“这个 class 的设计有没有考虑到 xxx 的问题”\\n\" +\n",
      "      \"“解析这个 class 是否有安全⻛险”\\n\" +\n",
      "      \"“...， 在哪些场景场景在可能会有泄露⻛险”\\n\" +\n",
      "      \"“这个 class 如何针对 xxx 做优化”\\n\" +\n",
      "      \"注意，一般直接问可能并不能拿到高质量的回答，需要人类做方向性的引导，比如提\\n\" +\n",
      "      \"示在什么问题、什么方面等 prompt，可以帮助 llm 沿着具体思路思考\\n\" +\n",
      "      \"并且要灵活使用 “给我 5 个 xx，并详细解释原因”\\n\" +\n",
      "      \"9. 灵活使用 cmd+i\\n\" +\n",
      "      \"最新的 copilot 支持了直接在代码上唤起 chat，你可以选中一段代码，然后 cmd +\\n\" +\n",
      "      \"i，输出你的 prompt，比如 “使用 promise.all 改写” “添加类型注释”\\n\" +\n",
      "      \"这个很多人没注意到这个功能，结合前面提到的 prompt 技巧很好用。\\n\" +\n",
      "      \"但目前 diff 功能有些 bug，在部分时候会删改不需要的代码，注意灵活应对。\\n\" +\n",
      "      \"我一般是把需要代码复制出来，然后 ctrl z 掉他所有更改，然后再粘贴进去。\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot8/14\\n\" +\n",
      "      \"因为这个功能没有上下文，但也有多次对话的能力。适合比较小的需求点，大的最好\\n\" +\n",
      "      \"是用 copliot chat\\n\" +\n",
      "      \"10. 写 commit message\\n\" +\n",
      "      \"这个已经在最新的 vsc 中集成，根据你本次的 diff 生成 commit message。\\n\" +\n",
      "      \"这个思路非常好，但实测其⻛格不太符合我日常的⻛格，我相信这个未来会有⻛格选\\n\" +\n",
      "      \"型，或者以你之前的 commit message 作为上下文进行生成。\\n\" +\n",
      "      \"目前我推荐在这个 generate 的基础上自己修改，或通过 chat 的方式生成\\n\" +\n",
      "      \"11. 基础脚手架、基础 poc\\n\" +\n",
      "      \"这也是 ai-native 的一部分，也是我最近用起来比较顺手的\\n\" +\n",
      "      \"“我要写一个 nodejs 库，帮我写 一个基础的 rollup 配置、tsconfig 和 package.json\\n\" +\n",
      "      \"的配置” “帮我用 react 写一个基础的 xxx 组件”\\n\" +\n",
      "      \"前者是，很多时候没有好用的现成配置，用 llm 就很方便。后者是有一个迅速能看到\\n\" +\n",
      "      \"的基础代码，会帮助你思考和工作。\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot9/14\\n\" +\n",
      "      \"12. 中间插入一些唠叨\\n\" +\n",
      "      \"vsc 设置成你最熟悉的自然语言！\\n\" +\n",
      "      \"虽然未来（或者已经）会有给 copilot chat 单独设置自然语言的功能，但我建议直接\\n\" +\n",
      "      \"把 vsc 设置成你最熟悉的自然语言。然后方便的速读\\n\" +\n",
      "      \"llm 不是人类，不用字斟句酌，有合适的关键词即可。如果不知道怎么表达，就用最\\n\" +\n",
      "      \"暴力的表达方式即可\\n\" +\n",
      "      \"写 prompt 的时间 和 写 code 的时间，这两个随着深入使用，你会逐步找到自己舒服\\n\" +\n",
      "      \"的状态。当你做一个事情的时候，你会知道是使用 llm 还是直接写 code 会更快/质量\\n\" +\n",
      "      \"更高。\\n\" +\n",
      "      \"一般理想的是 人类冷启动/llm 冷启动 => 人类编写细节/llm 编写细节 => 人类 polish /\\n\" +\n",
      "      \"llm polish\\n\" +\n",
      "      \"熟练后，在每个阶段都可以非常快速的判断出，这个时候是人类做还是 llm 做，还是\\n\" +\n",
      "      \"一起做\\n\" +\n",
      "      \"13. llm as doc/search\\n\" +\n",
      "      \"再强调，一定要问 llm 自己能够判断基础对错的问题！\\n\" +\n",
      "      \"这里的工具就不限于的 copilot chat 了，我一般也会混着 new bing （有联网能力）\\n\" +\n",
      "      \"使用。\\n\" +\n",
      "      \"比如\\n\" +\n",
      "      \"“ts 中，interface 和 type 的区别”\\n\" +\n",
      "      \"“ts decorators 是否 stable？现在进入 stage 几了？”（new bing）\\n\" +\n",
      "      \"几个非常好用的 magic word：“举例详细说明”、“详细对比这两个的优缺点”、“举出\\n\" +\n",
      "      \"实际场景对比这两个区别”、“使用 xxx 函数，写一个简单 demo，介绍其优势”\\n\" +\n",
      "      \"作完调研后，“用 xx 实现我的 xx 需求”，从调研到实现，几分钟 几轮对话，就结束了\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot10/14\\n\" +\n",
      "      \"一定要有基础的技术视野和知识去判断其输出的质量。我遇到过好几次，llm 硬着脖\\n\" +\n",
      "      \"子非要用 moment 去处理 ts 中的时间，直接被我喷回去，然后乖乖用 dayjs 了 \\n\" +\n",
      "      \"14. 碎碎念\\n\" +\n",
      "      \"我因为开源项目，一直可以免费用 copilot，算是非常老的用户了\\n\" +\n",
      "      \"之前比较流行的写注释然后让 copilot 补全代码的模式一直不太会用，会让我觉得很\\n\" +\n",
      "      \"怪。但 copilot chat 确实是 game changer，几天内就替代了我 50% 的工作。\\n\" +\n",
      "      \"我相信下一次⻜跃就是 copilot 带联网功能的时候，到时候会进一步挤压人类的编码\\n\" +\n",
      "      \"空间，亦或是说，人类可以更从容的做更有创造性的工作。\\n\" +\n",
      "      \"顺嘴提一句 copilot labs，可能很多人都不知道有这个东⻄。 在 chat 出来之前玩玩还\\n\" +\n",
      "      \"可以，在 chat 面前一文不值的 \\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot11/14\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot12/14\\n\" +\n",
      "      \"15. vsc plugin 开发\\n\" +\n",
      "      \"这也是我看很多人没提到的点，我日常工作有 vsc plugin 的开发工作，copilot chat\\n\" +\n",
      "      \"已经内置了 plugin 相关的文档，你可以直接用自然语言提问你的问题和需要开发的功\\n\" +\n",
      "      \"能在 vsc 中如何实现。\\n\" +\n",
      "      \"也可以通过 /help 命令，看看 chat 内置的一些功能，这些功能往往伴随着内置的\\n\" +\n",
      "      \"prompt 和数据库，对特定任务有增强\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot13/14\\n\" +\n",
      "      \"© 2024 Kai\\n\" +\n",
      "      \"\\n\" +\n",
      "      \"2024/3/24 20:59\\n\" +\n",
      "      \"如何使用 github copilot 完成 50% 的日常工作\\n\" +\n",
      "      \"https://kaiyi.cool/blog/github-copilot14/14\\n\" +\n",
      "      \"鲁ICP备2022030649号\",\n",
      "    metadata: {\n",
      "      source: \"data/github-copliot.pdf\",\n",
      "      pdf: {\n",
      "        version: \"1.10.100\",\n",
      "        info: {\n",
      "          PDFFormatVersion: \"1.4\",\n",
      "          IsAcroFormPresent: false,\n",
      "          IsXFAPresent: false,\n",
      "          Title: \"如何使用 github copilot 完成 50% 的日常工作\",\n",
      "          Creator: \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
      "          Producer: \"Skia/PDF m123\",\n",
      "          CreationDate: \"D:20240324125917+00'00'\",\n",
      "          ModDate: \"D:20240324125917+00'00'\"\n",
      "        },\n",
      "        metadata: null,\n",
      "        totalPages: 14\n",
      "      }\n",
      "    },\n",
      "    id: undefined\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import * as pdfParse from \"pdf-parse\";\n",
    "import { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\n",
    "const loader = new PDFLoader(\"data/github-copliot.pdf\", { splitPages: false });\n",
    "const pdf = await loader.load()\n",
    "\n",
    "console.log(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { DirectoryLoader } from \"langchain/document_loaders/fs/directory\";\n",
    "\n",
    "const loader = new DirectoryLoader(\n",
    "  \"./data\",\n",
    "  {\n",
    "    \".pdf\": (path) => new PDFLoader(path, { splitPages: false }),\n",
    "    \".txt\": (path) => new TextLoader(path),\n",
    "  }\n",
    ");\n",
    "const docs = await loader.load();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not find package 'ignore' from referrer 'file:///Users/huccct/Frontend/learn-langchainjs/node_modules/.deno/langchain@0.1.29/node_modules/langchain/dist/document_loaders/web/github.js'.",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "TypeError: Could not find package 'ignore' from referrer 'file:///Users/huccct/Frontend/learn-langchainjs/node_modules/.deno/langchain@0.1.29/node_modules/langchain/dist/document_loaders/web/github.js'.",
      "    at async <anonymous>:1:51"
     ]
    }
   ],
   "source": [
    "import { GithubRepoLoader } from \"langchain/document_loaders/web/github\";\n",
    "import ignore from \"ignore\";\n",
    "\n",
    "const loader = new GithubRepoLoader(\n",
    "    \"https://github.com/RealKai42/qwerty-learner\",\n",
    "    { \n",
    "        branch: \"master\",\n",
    "        recursive: false, \n",
    "        unknown: \"warn\", \n",
    "        ignorePaths: [\"*.md\", \"yarn.lock\", \"*.json\"],\n",
    "        accessToken: env[\"GITHUB_TOKEN\"]\n",
    "    }\n",
    "  );\n",
    "\n",
    "console.log(loader);\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot read properties of undefined (reading 'node')",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "TypeError: Cannot read properties of undefined (reading 'node')",
      "    at Object.<anonymous> (file:///Users/huccct/Frontend/learn-langchainjs/node_modules/.deno/undici@6.21.1/node_modules/undici/lib/core/util.js:16:49)",
      "    at Object.<anonymous> (file:///Users/huccct/Frontend/learn-langchainjs/node_modules/.deno/undici@6.21.1/node_modules/undici/lib/core/util.js:721:4)",
      "    at Module._compile (node:module:745:34)",
      "    at loadMaybeCjs (node:module:770:10)",
      "    at Object.Module._extensions..js (node:module:755:12)",
      "    at Module.load (node:module:662:32)",
      "    at Function.Module._load (node:module:534:12)",
      "    at Module.require (node:module:681:19)",
      "    at require (node:module:812:16)",
      "    at Object.<anonymous> (file:///Users/huccct/Frontend/learn-langchainjs/node_modules/.deno/undici@6.21.1/node_modules/undici/lib/dispatcher/client.js:8:14)"
     ]
    }
   ],
   "source": [
    "import { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\n",
    "import \"cheerio\";\n",
    "\n",
    "// const loader = new CheerioWebBaseLoader(\n",
    "//   \"https://kaiyi.cool/blog/github-copilot\"\n",
    "// );\n",
    "\n",
    "// const docs = await loader.load();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  Document {\n",
      "    pageContent: '{\"type\":\"organic_result\",\"title\":\"有关使用GitHub Copilot 的最佳做法\",\"link\":\"https://docs.github.com/zh/copilot/using-github-copilot/best-practices-for-using-github-copilot#:~:text=GitHub%20Copilot%20%E6%98%AF%E4%B8%80%E6%AC%BE,%E6%B5%8B%E8%AF%95%E5%92%8C%E9%87%8D%E5%A4%8D%E6%80%A7%E4%BB%A3%E7%A0%81\",\"displayed_link\":\"https://docs.github.com › copilot › using-github-copilot\",\"snippet\":\"GitHub Copilot 是一款AI 编码助手，可帮助你更快、更省力地编写代码，从而将更多精力集中在问题解决和协作上。 在开始使用Copilot 之前，请务必了解何时应使用以及何时不应使用此功能。 Copilot 最擅长的部分能力包括： 编写测试和重复性代码\",\"snippet_highlighted_words\":[\"是一款AI 编码助手，可帮助你更快、更省力地编写代码，从而将更多精力集中在问题解决和协作上\"],\"favicon\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/ddf35b21095b25aa0e374bb30fccb487d5eb98f3a8c7a7fc.png\",\"source\":\"GitHub Docs\"}',\n",
      "    metadata: { source: \"SerpAPI\", responseType: \"answer_box\" }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: '{\"title\":\"GitHub Copilot\",\"type\":\"Software\",\"entity_type\":\"webanswers\",\"kgmid\":\"/g/11q83qbj3d\",\"knowledge_graph_search_link\":\"https://www.google.com/search?kgmid=/g/11q83qbj3d&hl=en-US&q=GitHub+Copilot\",\"serpapi_knowledge_graph_search_link\":\"https://serpapi.com/search.json?device=desktop&engine=google&google_domain=google.com&hl=en-US&kgmid=%2Fg%2F11q83qbj3d&q=GitHub+Copilot\",\"header_images\":[{\"image\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/6d6d037327de291e22cdba0525a7c81236fa57b107f025efb40bac567b6000fe7d021af5e1cf22a3.webp\"},{\"image\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/6d6d037327de291e22cdba0525a7c81236fa57b107f025efd70e89082f092172d7799f67908a5696.webp\"},{\"image\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/6d6d037327de291e22cdba0525a7c81236fa57b107f025efb9cf4c156ccf09d4dc62a9ee672312e9.webp\"},{\"image\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/6d6d037327de291e22cdba0525a7c81236fa57b107f025ef683f2febd5b1653e3d0540921d389650.webp\"}],\"initial_release_date\":\"October 2021\",\"developer\":\"GitHub, OpenAI\",\"developer_links\":[{\"text\":\"GitHub\",\"link\":\"https://www.google.com/search?sca_esv=971b96cac2f4b729&q=GitHub&si=APYL9btKi1TLoawpxIKkhA47KIc3RH36yjJAdk2TmwBtOZld-h4RhJ6UwsPKRLJkC-3smOAOdUsy79ojFJ5pEL4slXuzFQD7KgATGbuoWPGNGjpMiPeEAFBKWUbDXdIFG-JnMhxJNwwDgS1AeCtOlQ7qhPtg_lYK1a69QEJ2pLsSaAhiA8WlMMw2AkR1Lojbx5ME_PbwBCCB60ORv9SG7X0Mrnob8L_O1aQuOvRPwEmQsHr0Jm-KZf4RMv7R4oIRz1ed7hLgIE2WM6AJ-1EQ6JKiLluwi-tCsw%3D%3D&sa=X&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQmxMoAHoECDkQAg\"},{\"text\":\"OpenAI\",\"link\":\"https://www.google.com/search?sca_esv=971b96cac2f4b729&q=OpenAI&si=APYL9bvbTYBlvjo9HgsKokb80VOuw9zV-z5EXyhbMKCadi8Rh3cutGsMp3cZ23bOn6nDAC6Cg6rh8JynTcJFB-dTJSSqqnyuukqR8_F9575ujFe70AnntW36CnWa9SQPVdIpwrl2GKA6kv4uI0_O6jdj-W1nxhc67KioGxtqdeXnwbIoxP8xgswrZ2K7X1gZVMFrSsY_DWhvh_KfT4D2wwQucubgulo-J9FOVuzEAn3JOOjW152k9zrVSgj33RHV0yBhGH1vIZPa7dJHeMFoH9MY-k-9rQ5VpA%3D%3D&sa=X&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQmxMoAXoECDkQAw\"}],\"operating_system\":\"Microsoft Windows, Linux, macOS, Web\",\"operating_system_links\":[{\"text\":\"Microsoft Windows\",\"link\":\"https://www.google.com/search?sca_esv=971b96cac2f4b729&q=Microsoft+Windows&stick=H4sIAAAAAAAAAONgVuLQz9U3MCmKt1jEKuibmVyUX5yfVqIQnpmXkl9eDABrmtGqIAAAAA&sa=X&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQmxMoAHoECDgQAg\"},{\"text\":\"Linux\",\"link\":\"https://www.google.com/search?sca_esv=971b96cac2f4b729&q=Linux&stick=H4sIAAAAAAAAAONgVuLUz9U3SCuoqipYxMrqk5lXWgEATgerNhUAAAA&sa=X&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQmxMoAXoECDgQAw\"},{\"text\":\"macOS\",\"link\":\"https://www.google.com/search?sca_esv=971b96cac2f4b729&q=macOS&stick=H4sIAAAAAAAAAONgVuLQz9U3MDWtLFrEypqbmOwfDADb5b_yFAAAAA&sa=X&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQmxMoAnoECDgQBA\"}],\"stable_release\":\"1.7.4421\",\"web_results\":[{\"title\":\"MoreLess\",\"link\":\"https://en.wikipedia.org/wiki/GitHub_Copilot\"}]}',\n",
      "    metadata: { source: \"SerpAPI\", responseType: \"knowledge_graph\" }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: '{\"position\":1,\"title\":\"什么是GitHub Copilot？ - GitHub 文档\",\"link\":\"https://docs.github.com/zh/copilot/about-github-copilot/what-is-github-copilot\",\"redirect_link\":\"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://docs.github.com/zh/copilot/about-github-copilot/what-is-github-copilot&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQFnoECBAQAQ\",\"displayed_link\":\"https://docs.github.com › copilot\",\"favicon\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/82277c65fb25b02675bdb085dc6a1332ae409164795d97236ae0a852bef5c20c.png\",\"snippet\":\"GitHub Copilot 是一款AI 编码助手，可帮助你更快、更省力地编写代码，从而将更多精力集中在问题解决和协作上。 GitHub Copilot 已被证明可以提高开发人员的工作效率并 ...\",\"snippet_highlighted_words\":[\"GitHub Copilot 是一款AI 编码助手\"],\"source\":\"GitHub Docs\"}',\n",
      "    metadata: { source: \"SerpAPI\", responseType: \"organic_results\" }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: '{\"position\":2,\"title\":\"GitHub Copilot 简介- Training\",\"link\":\"https://learn.microsoft.com/zh-cn/training/modules/introduction-to-github-copilot/\",\"redirect_link\":\"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://learn.microsoft.com/zh-cn/training/modules/introduction-to-github-copilot/&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQFnoECA4QAQ\",\"displayed_link\":\"https://learn.microsoft.com › modules\",\"favicon\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/82277c65fb25b02675bdb085dc6a1332fa12f9da35306d02f6995b4943c6143b.png\",\"snippet\":\"GitHub Copilot 是一个AI 结对程序员，可在编写代码时提供自动完成样式的建议。\",\"snippet_highlighted_words\":[\"GitHub Copilot\"],\"source\":\"Learn Microsoft\"}',\n",
      "    metadata: { source: \"SerpAPI\", responseType: \"organic_results\" }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: '{\"position\":3,\"title\":\"使用GitHub Copilot Visual Studio - AI 结对编程\",\"link\":\"https://visualstudio.microsoft.com/zh-hans/github-copilot/\",\"redirect_link\":\"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://visualstudio.microsoft.com/zh-hans/github-copilot/&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQFnoECA8QAQ\",\"displayed_link\":\"https://visualstudio.microsoft.com › ...\",\"favicon\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/82277c65fb25b02675bdb085dc6a13324a2fbe8e5b8ee1c4f560cf0c1dfffb4c.png\",\"date\":\"Feb 10, 2025\",\"snippet\":\"GitHub Copilot 是由人工智能(AI) 提供支持的编码助手，可在各种环境中运行，帮助你更高效地完成日常编码任务。 在此新系列内容中，我们将专门介绍GitHub ...\",\"snippet_highlighted_words\":[\"GitHub Copilot\",\"GitHub\"],\"source\":\"Visual Studio\"}',\n",
      "    metadata: { source: \"SerpAPI\", responseType: \"organic_results\" }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: '{\"position\":4,\"title\":\"GitHub Copilot 功能 - GitHub 文档\",\"link\":\"https://docs.github.com/zh/copilot/about-github-copilot/github-copilot-features\",\"redirect_link\":\"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://docs.github.com/zh/copilot/about-github-copilot/github-copilot-features&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQFnoECE8QAQ\",\"displayed_link\":\"https://docs.github.com › copilot › github-copilot-featu...\",\"favicon\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/82277c65fb25b02675bdb085dc6a1332cc21f1ee375170f02f582aae9dffafbf.png\",\"snippet\":\"A chat-like interface in the terminal, where you can ask questions about the command line. You can ask Copilot to provide command suggestions or explanations of ...\",\"snippet_highlighted_words\":[\"interface\"],\"source\":\"GitHub Docs\"}',\n",
      "    metadata: { source: \"SerpAPI\", responseType: \"organic_results\" }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: `{\"position\":5,\"title\":\"GitHub Copilot 快速入门\",\"link\":\"https://docs.github.com/zh/copilot/quickstart\",\"redirect_link\":\"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://docs.github.com/zh/copilot/quickstart&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQFnoECFQQAQ\",\"displayed_link\":\"https://docs.github.com › quickstart\",\"thumbnail\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSAH1wye1rGT-_qmF9tRiWVgMrKztzsHNyRt3ob-7k&usqp=CAE&s\",\"favicon\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/82277c65fb25b02675bdb085dc6a133225370200e2c5ba777d95bce87ada8de9.png\",\"snippet\":\"You can use Copilot to get answers to coding-related questions, such as how best to code something, how to fix a bug, or how someone else's code works.\",\"snippet_highlighted_words\":[\"Copilot\"],\"source\":\"GitHub Docs\"}`,\n",
      "    metadata: { source: \"SerpAPI\", responseType: \"organic_results\" }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: '{\"position\":6,\"title\":\"GitHub Copilot 文档\",\"link\":\"https://docs.github.com/zh/copilot\",\"redirect_link\":\"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://docs.github.com/zh/copilot&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQFnoECFYQAQ\",\"displayed_link\":\"https://docs.github.com › copilot\",\"favicon\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/82277c65fb25b02675bdb085dc6a13326458a3dff24d8ee9553c39d67faefd3c.png\",\"snippet\":\"可以使用GitHub Copilot 在编写代码时从AI 配对程序员那里获取自动完成风格的建议。 概述快速入门. 从此处开始. 什么是GitHub Copilot？ 了解GitHub Copilot 及其用途。\",\"snippet_highlighted_words\":[\"可以使用GitHub Copilot 在编写代码时从AI 配对程序员那里获取自动完成风格的建议\"],\"source\":\"GitHub Docs\"}',\n",
      "    metadata: { source: \"SerpAPI\", responseType: \"organic_results\" }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: '{\"position\":7,\"title\":\"GitHub Copilot - 维基百科，自由的百科全书\",\"link\":\"https://zh.wikipedia.org/zh-hans/GitHub_Copilot\",\"redirect_link\":\"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://zh.wikipedia.org/zh-hans/GitHub_Copilot&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQFnoECFAQAQ\",\"displayed_link\":\"https://zh.wikipedia.org › zh-hans\",\"favicon\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/82277c65fb25b02675bdb085dc6a1332ecda5cf3a255b9def419184dfa6fa983.png\",\"snippet\":\"GitHub Copilot是GitHub和OpenAI合作开发的一个人工智能工具，用户在使用Visual Studio Code、Microsoft Visual Studio、Vim、Cursor或JetBrains集成开发环境时可以 ...\",\"snippet_highlighted_words\":[\"GitHub Copilot\",\"GitHub\"],\"source\":\"维基百科\"}',\n",
      "    metadata: { source: \"SerpAPI\", responseType: \"organic_results\" }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: '{\"position\":8,\"title\":\"关于GitHub Copilot - GitHub 文档\",\"link\":\"https://docs.github.com/zh/copilot/about-github-copilot\",\"redirect_link\":\"https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://docs.github.com/zh/copilot/about-github-copilot&ved=2ahUKEwjW_sfTwPuLAxWx4jgGHZOwDrQQFnoECE4QAQ\",\"displayed_link\":\"https://docs.github.com › copilot\",\"favicon\":\"https://serpapi.com/searches/67ccc0e41240097f4192e2e4/images/82277c65fb25b02675bdb085dc6a1332847eb7db33ed65b50f6ffe7a231bd5b6.png\",\"snippet\":\"GitHub Copilot 包括一套功能。 Copilot 还为管理员提供一套功能。 GitHub Copilot 的订阅计划. 了解Copilot 的订阅选项。 帮助和 ...\",\"snippet_highlighted_words\":[\"GitHub Copilot\"],\"source\":\"GitHub Docs\"}',\n",
      "    metadata: { source: \"SerpAPI\", responseType: \"organic_results\" }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { SerpAPILoader } from \"langchain/document_loaders/web/serpapi\";\n",
    "\n",
    "const apiKey = env[\"SERP_KEY\"]\n",
    "const question = \"什么 github copliot\"\n",
    "const loader = new SerpAPILoader({ q: question, apiKey });\n",
    "const docs = await loader.load();\n",
    "console.log(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  Document {\n",
      "    pageContent: \"鲁镇的酒店的格局，是和别处不同的：都是当街一个曲尺形的大柜台，柜里面预备着热水，可以随时温酒。做工的人，傍午傍晚散了工，每每花四\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 1, to: 1 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"工的人，傍午傍晚散了工，每每花四文铜钱，买一碗酒，——这是二十多年前的事，现在每碗要涨到十文，——靠柜外站着，热热的喝了休息；倘\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 1, to: 1 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"—靠柜外站着，热热的喝了休息；倘肯多花一文，便可以买一碟盐煮笋，或者茴香豆，做下酒物了，如果出到十几文，那就能买一样荤菜，但这些\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 1, to: 1 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"十几文，那就能买一样荤菜，但这些顾客，多是短衣帮，大抵没有这样阔绰。只有穿长衫的，才踱进店面隔壁的房子里，要酒要菜，慢慢地坐喝。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 1, to: 1 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"我从十二岁起，便在镇口的咸亨酒店里当伙计，掌柜说，我样子太傻，怕侍候不了长衫主顾，就在外面做点事罢。外面的短衣主顾，虽然容易说\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 3, to: 3 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"事罢。外面的短衣主顾，虽然容易说话，但唠唠叨叨缠夹不清的也很不少。他们往往要亲眼看着黄酒从坛子里舀出，看过壶子底里有水没有，又亲\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 3, to: 3 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"舀出，看过壶子底里有水没有，又亲看将壶子放在热水里，然后放心：在这严重监督下，羼水也很为难。所以过了几天，掌柜又说我干不了这事。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 3, to: 3 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"过了几天，掌柜又说我干不了这事。幸亏荐头的情面大，辞退不得，便改为专管温酒的一种无聊职务了。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 3, to: 3 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"我从此便整天的站在柜台里，专管我的职务。虽然没有什么失职，但总觉得有些单调，有些无聊。掌柜是一副凶脸孔，主顾也没有好声气，教人\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 5, to: 5 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"副凶脸孔，主顾也没有好声气，教人活泼不得；只有孔乙己到店，才可以笑几声，所以至今还记得。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 5, to: 5 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"孔乙己是站着喝酒而穿长衫的唯一的人。他身材很高大；青白脸色，皱纹间时常夹些伤痕；一部乱蓬蓬的花白的胡子。穿的虽然是长衫，可是又\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"白的胡子。穿的虽然是长衫，可是又脏又破，似乎十多年没有补，也没有洗。他对人说话，总是满口之乎者也，叫人半懂不懂的。因为他姓孔，别\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"，叫人半懂不懂的。因为他姓孔，别人便从描红纸上的“上大人孔乙己”这半懂不懂的话里，替他取下一个绰号，叫作孔乙己。孔乙己一到店，所\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"号，叫作孔乙己。孔乙己一到店，所有喝酒的人便都看着他笑，有的叫道，“孔乙己，你脸上又添上新伤疤了！”他不回答，对柜里说，“温两碗\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"！”他不回答，对柜里说，“温两碗酒，要一碟茴香豆。”便排出九文大钱。他们又故意的高声嚷道，“你一定又偷了人家的东西了！”孔乙己睁\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"定又偷了人家的东西了！”孔乙己睁大眼睛说，“你怎么这样凭空污人清白……”“什么清白？我前天亲眼见你偷了何家的书，吊着打。”孔乙己\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"你偷了何家的书，吊着打。”孔乙己便涨红了脸，额上的青筋条条绽出，争辩道，“窃书不能算偷……窃书！……读书人的事，能算偷么？”接连\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"……读书人的事，能算偷么？”接连便是难懂的话，什么“君子固穷”，什么“者乎”之类，引得众人都哄笑起来：店内外充满了快活的空气。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"听人家背地里谈论，孔乙己原来也读过书，但终于没有进学，又不会营生；于是愈过愈穷，弄到将要讨饭了。幸而写得一笔好字，便替人家抄抄\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 9, to: 9 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"。幸而写得一笔好字，便替人家抄抄书，换一碗饭吃。可惜他又有一样坏脾气，便是好喝懒做。坐不到几天，便连人和书籍纸张笔砚，一齐失踪。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 9, to: 9 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"便连人和书籍纸张笔砚，一齐失踪。如是几次，叫他抄书的人也没有了。孔乙己没有法，便免不了偶然做些偷窃的事。但他在我们店里，品行却比\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 9, to: 9 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"窃的事。但他在我们店里，品行却比别人都好，就是从不拖欠；虽然间或没有现钱，暂时记在粉板上，但不出一月，定然还清，从粉板上拭去了孔\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 9, to: 9 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"一月，定然还清，从粉板上拭去了孔乙己的名字。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 9, to: 9 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"孔乙己喝过半碗酒，涨红的脸色渐渐复了原，旁人便又问道，“孔乙己，你当真认识字么？”孔乙己看着问他的人，显出不屑置辩的神气。他们\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 11, to: 11 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"他的人，显出不屑置辩的神气。他们便接着说道，“你怎的连半个秀才也捞不到呢？”孔乙己立刻显出颓唐不安模样，脸上笼上了一层灰色，嘴里\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 11, to: 11 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"安模样，脸上笼上了一层灰色，嘴里说些话；这回可是全是之乎者也之类，一些不懂了。在这时候，众人也都哄笑起来：店内外充满了快活的空气\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 11, to: 11 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"哄笑起来：店内外充满了快活的空气。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 11, to: 11 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"“多乎哉?不多也。”\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 13, to: 13 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"有几回，邻居孩子听得笑声，也赶热闹，围住了孔乙己。他便给他们一人一颗。孩子吃完豆，仍然不散，眼睛都望着碟子。孔乙己着了慌，伸开\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 15, to: 15 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"睛都望着碟子。孔乙己着了慌，伸开五指将碟子罩住，弯腰下去说道，“不多了，我已经不多了。”直起身又看一看豆，自己摇头说，“不多不多\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 15, to: 15 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"看一看豆，自己摇头说，“不多不多！多乎哉？不多也。”于是这一群孩子都在笑声里走散了。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 15, to: 15 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"孔乙己是这样的使人快活，可是没有他，别人也便这么过。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 17, to: 17 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"有一天，大约是中秋前的两三天，掌柜正在慢慢的结账，取下粉板，忽然说，“孔乙己长久没有来了。还欠十九个钱呢！”我才也觉得他的确长\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 19, to: 19 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"十九个钱呢！”我才也觉得他的确长久没有来了。一个喝酒的人说道，“他怎么会来？……他打折了腿了。”掌柜说，“哦！”“他总仍旧是偷。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 19, to: 19 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"掌柜说，“哦！”“他总仍旧是偷。这一回，是自己发昏，竟偷到丁举人家里去了。他家的东西，偷得的吗？”“后来怎么样？”“怎么样？先写\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 19, to: 19 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"”“后来怎么样？”“怎么样？先写服辩，后来是打，打了大半夜，再打折了腿。”“后来呢？”“后来打折了腿了。”“打折了怎样呢？”“怎\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 19, to: 19 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"了腿了。”“打折了怎样呢？”“怎样？……谁晓得？许是死了。”掌柜也不再问，仍然慢慢的算他的账。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 19, to: 19 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"中秋过后，秋风是一天凉比一天，看看将近初冬；我整天的靠着火，也须穿上棉袄了。一天的下半天，没有一个顾客，我正合了眼坐着。忽然间\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"一个顾客，我正合了眼坐着。忽然间听得一个声音，“温一碗酒。”这声音虽然极低，却很耳熟。看时又全没有人。站起来向外一望，那孔乙己便\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"有人。站起来向外一望，那孔乙己便在柜台下对了门槛坐着。他脸上黑而且瘦，已经不成样子；穿一件破夹袄，盘着两腿，下面垫一个蒲包，用草\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"，盘着两腿，下面垫一个蒲包，用草绳在肩上挂住；见了我，又说道，“温一碗酒。”掌柜也伸出头去，一面说，“孔乙己么？你还欠十九个钱呢\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"说，“孔乙己么？你还欠十九个钱呢！”孔乙己很颓唐的仰面答道，“这……下回还清罢。这一回是现钱，酒要好。”掌柜仍然同平常一样，笑着\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"要好。”掌柜仍然同平常一样，笑着对他说，“孔乙己，你又偷了东西了！”但他这回却不十分分辩，单说了一句“不要取笑！”“取笑？要是不\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"一句“不要取笑！”“取笑？要是不偷，怎么会打断腿？”孔乙己低声说道，“跌断，跌，跌……”他的眼色，很像恳求掌柜，不要再提。此时已\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"，很像恳求掌柜，不要再提。此时已经聚集了几个人，便和掌柜都笑了。我温了酒，端出去，放在门槛上。他从破衣袋里摸出四文大钱，放在我手\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"从破衣袋里摸出四文大钱，放在我手里，见他满手是泥，原来他便用这手走来的。不一会，他喝完酒，便又在旁人的说笑声中，坐着用这手慢慢走\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"旁人的说笑声中，坐着用这手慢慢走去了。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"自此以后，又长久没有看见孔乙己。到了年关，掌柜取下粉板说，“孔乙己还欠十九个钱呢！”到第二年的端午，又说“孔乙己还欠十九个钱呢\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 23, to: 23 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"端午，又说“孔乙己还欠十九个钱呢！”到中秋可是没有说，再到年关也没有看见他。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 23, to: 23 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"我到现在终于没有见——大约孔乙己的确死了。s\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 25, to: 25 } } }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
    "import { TextLoader } from \"langchain/document_loaders/fs/text\";\n",
    "\n",
    "const loader = new TextLoader(\"data/kong.txt\");\n",
    "const docs = await loader.load();\n",
    "\n",
    "const splitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 64,\n",
    "    chunkOverlap: 16,\n",
    "  });\n",
    "\n",
    "\n",
    "\n",
    "const splitDocs = await splitter.splitDocuments(docs);\n",
    "console.log(splitDocs);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"cpp\",      \"go\",\n",
      "  \"java\",     \"js\",\n",
      "  \"php\",      \"proto\",\n",
      "  \"python\",   \"rst\",\n",
      "  \"ruby\",     \"rust\",\n",
      "  \"scala\",    \"swift\",\n",
      "  \"markdown\", \"latex\",\n",
      "  \"html\",     \"sol\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { SupportedTextSplitterLanguages } from \"langchain/text_splitter\";\n",
    "\n",
    "console.log(SupportedTextSplitterLanguages); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  Document {\n",
      "    pageContent: \"function myFunction(name,job){\",\n",
      "    metadata: { loc: { lines: { from: 2, to: 2 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'console.log(\"Welcome \" + name + \", the \" + job);\\n}',\n",
      "    metadata: { loc: { lines: { from: 3, to: 4 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"myFunction('Harry Potter','Wizard')\",\n",
      "    metadata: { loc: { lines: { from: 6, to: 6 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"function forFunction(){\\n\\tfor (let i=0; i<5; i++){\",\n",
      "    metadata: { loc: { lines: { from: 8, to: 9 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'console.log(\"这个数字是\" + i)\\n\\t}\\n}',\n",
      "    metadata: { loc: { lines: { from: 10, to: 12 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"forFunction()\",\n",
      "    metadata: { loc: { lines: { from: 14, to: 14 } } }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
    "\n",
    "const js = `\n",
    "function myFunction(name,job){\n",
    "\tconsole.log(\"Welcome \" + name + \", the \" + job);\n",
    "}\n",
    "\n",
    "myFunction('Harry Potter','Wizard')\n",
    "\n",
    "function forFunction(){\n",
    "\tfor (let i=0; i<5; i++){\n",
    "        console.log(\"这个数字是\" + i)\n",
    "\t}\n",
    "}\n",
    "\n",
    "forFunction()\n",
    "`;\n",
    "\n",
    "const splitter = RecursiveCharacterTextSplitter.fromLanguage(\"js\", {\n",
    "  chunkSize: 64,\n",
    "  chunkOverlap: 0,\n",
    "});\n",
    "const jsOutput = await splitter.createDocuments([js]);\n",
    "console.log(jsOutput);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  Document {\n",
      "    pageContent: \"I stand before you today the representative of a family\",\n",
      "    metadata: { loc: { lines: { from: 1, to: 1 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \" in grief, in a country in mourning before a\",\n",
      "    metadata: { loc: { lines: { from: 1, to: 1 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \" world in shock.\",\n",
      "    metadata: { loc: { lines: { from: 1, to: 1 } } }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { TokenTextSplitter } from \"langchain/text_splitter\";\n",
    "\n",
    "const text = \"I stand before you today the representative of a family in grief, in a country in mourning before a world in shock.\";\n",
    "\n",
    "const splitter = new TokenTextSplitter({\n",
    "  chunkSize: 10,\n",
    "  chunkOverlap: 0,\n",
    "});\n",
    "\n",
    "const docs = await splitter.createDocuments([text]);\n",
    "console.log(docs);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  Document {\n",
      "    pageContent: \"鲁镇的酒店的格局，是和别处不同的：都是当街一个曲尺形的大柜台，柜里面预备着热水，可以随时温酒。做工的人，傍午傍晚散了工，每每花四文铜钱，买一碗酒，——这是二十多年前的事，现在每碗要涨到十文，——靠柜外\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 1, to: 1 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"年前的事，现在每碗要涨到十文，——靠柜外站着，热热的喝了休息；倘肯多花一文，便可以买一碟盐煮笋，或者茴香豆，做下酒物了，如果出到十几文，那就能买一样荤菜，但这些顾客，多是短衣帮，大抵没有这样阔绰。只有\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 1, to: 1 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"顾客，多是短衣帮，大抵没有这样阔绰。只有穿长衫的，才踱进店面隔壁的房子里，要酒要菜，慢慢地坐喝。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 1, to: 1 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"我从十二岁起，便在镇口的咸亨酒店里当伙计，掌柜说，我样子太傻，怕侍候不了长衫主顾，就在外面做点事罢。外面的短衣主顾，虽然容易说话，但唠唠叨叨缠夹不清的也很不少。他们往往要亲眼看着黄酒从坛子里舀出，看\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 3, to: 3 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"。他们往往要亲眼看着黄酒从坛子里舀出，看过壶子底里有水没有，又亲看将壶子放在热水里，然后放心：在这严重监督下，羼水也很为难。所以过了几天，掌柜又说我干不了这事。幸亏荐头的情面大，辞退不得，便改为专管温\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 3, to: 3 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"幸亏荐头的情面大，辞退不得，便改为专管温酒的一种无聊职务了。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 3, to: 3 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"我从此便整天的站在柜台里，专管我的职务。虽然没有什么失职，但总觉得有些单调，有些无聊。掌柜是一副凶脸孔，主顾也没有好声气，教人活泼不得；只有孔乙己到店，才可以笑几声，所以至今还记得。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 5, to: 5 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"孔乙己是站着喝酒而穿长衫的唯一的人。他身材很高大；青白脸色，皱纹间时常夹些伤痕；一部乱蓬蓬的花白的胡子。穿的虽然是长衫，可是又脏又破，似乎十多年没有补，也没有洗。他对人说话，总是满口之乎者也，叫人半\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"洗。他对人说话，总是满口之乎者也，叫人半懂不懂的。因为他姓孔，别人便从描红纸上的“上大人孔乙己”这半懂不懂的话里，替他取下一个绰号，叫作孔乙己。孔乙己一到店，所有喝酒的人便都看着他笑，有的叫道，“孔乙\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"有喝酒的人便都看着他笑，有的叫道，“孔乙己，你脸上又添上新伤疤了！”他不回答，对柜里说，“温两碗酒，要一碟茴香豆。”便排出九文大钱。他们又故意的高声嚷道，“你一定又偷了人家的东西了！”孔乙己睁大眼睛说\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"定又偷了人家的东西了！”孔乙己睁大眼睛说，“你怎么这样凭空污人清白……”“什么清白？我前天亲眼见你偷了何家的书，吊着打。”孔乙己便涨红了脸，额上的青筋条条绽出，争辩道，“窃书不能算偷……窃书！……读书\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"争辩道，“窃书不能算偷……窃书！……读书人的事，能算偷么？”接连便是难懂的话，什么“君子固穷”，什么“者乎”之类，引得众人都哄笑起来：店内外充满了快活的空气。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"听人家背地里谈论，孔乙己原来也读过书，但终于没有进学，又不会营生；于是愈过愈穷，弄到将要讨饭了。幸而写得一笔好字，便替人家抄抄书，换一碗饭吃。可惜他又有一样坏脾气，便是好喝懒做。坐不到几天，便连人和\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 9, to: 9 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"脾气，便是好喝懒做。坐不到几天，便连人和书籍纸张笔砚，一齐失踪。如是几次，叫他抄书的人也没有了。孔乙己没有法，便免不了偶然做些偷窃的事。但他在我们店里，品行却比别人都好，就是从不拖欠；虽然间或没有现钱\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 9, to: 9 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"别人都好，就是从不拖欠；虽然间或没有现钱，暂时记在粉板上，但不出一月，定然还清，从粉板上拭去了孔乙己的名字。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 9, to: 9 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"孔乙己喝过半碗酒，涨红的脸色渐渐复了原，旁人便又问道，“孔乙己，你当真认识字么？”孔乙己看着问他的人，显出不屑置辩的神气。他们便接着说道，“你怎的连半个秀才也捞不到呢？”孔乙己立刻显出颓唐不安模样，\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 11, to: 11 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"捞不到呢？”孔乙己立刻显出颓唐不安模样，脸上笼上了一层灰色，嘴里说些话；这回可是全是之乎者也之类，一些不懂了。在这时候，众人也都哄笑起来：店内外充满了快活的空气。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 11, to: 11 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"“多乎哉?不多也。”\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 13, to: 13 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"有几回，邻居孩子听得笑声，也赶热闹，围住了孔乙己。他便给他们一人一颗。孩子吃完豆，仍然不散，眼睛都望着碟子。孔乙己着了慌，伸开五指将碟子罩住，弯腰下去说道，“不多了，我已经不多了。”直起身又看一看豆\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 15, to: 15 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"不多了，我已经不多了。”直起身又看一看豆，自己摇头说，“不多不多！多乎哉？不多也。”于是这一群孩子都在笑声里走散了。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 15, to: 15 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"孔乙己是这样的使人快活，可是没有他，别人也便这么过。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 17, to: 17 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"有一天，大约是中秋前的两三天，掌柜正在慢慢的结账，取下粉板，忽然说，“孔乙己长久没有来了。还欠十九个钱呢！”我才也觉得他的确长久没有来了。一个喝酒的人说道，“他怎么会来？……他打折了腿了。”掌柜说，\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 19, to: 19 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"他怎么会来？……他打折了腿了。”掌柜说，“哦！”“他总仍旧是偷。这一回，是自己发昏，竟偷到丁举人家里去了。他家的东西，偷得的吗？”“后来怎么样？”“怎么样？先写服辩，后来是打，打了大半夜，再打折了腿。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 19, to: 19 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"服辩，后来是打，打了大半夜，再打折了腿。”“后来呢？”“后来打折了腿了。”“打折了怎样呢？”“怎样？……谁晓得？许是死了。”掌柜也不再问，仍然慢慢的算他的账。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 19, to: 19 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"中秋过后，秋风是一天凉比一天，看看将近初冬；我整天的靠着火，也须穿上棉袄了。一天的下半天，没有一个顾客，我正合了眼坐着。忽然间听得一个声音，“温一碗酒。”这声音虽然极低，却很耳熟。看时又全没有人。站\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"音虽然极低，却很耳熟。看时又全没有人。站起来向外一望，那孔乙己便在柜台下对了门槛坐着。他脸上黑而且瘦，已经不成样子；穿一件破夹袄，盘着两腿，下面垫一个蒲包，用草绳在肩上挂住；见了我，又说道，“温一碗酒\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"绳在肩上挂住；见了我，又说道，“温一碗酒。”掌柜也伸出头去，一面说，“孔乙己么？你还欠十九个钱呢！”孔乙己很颓唐的仰面答道，“这……下回还清罢。这一回是现钱，酒要好。”掌柜仍然同平常一样，笑着对他说，\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"要好。”掌柜仍然同平常一样，笑着对他说，“孔乙己，你又偷了东西了！”但他这回却不十分分辩，单说了一句“不要取笑！”“取笑？要是不偷，怎么会打断腿？”孔乙己低声说道，“跌断，跌，跌……”他的眼色，很像恳\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"道，“跌断，跌，跌……”他的眼色，很像恳求掌柜，不要再提。此时已经聚集了几个人，便和掌柜都笑了。我温了酒，端出去，放在门槛上。他从破衣袋里摸出四文大钱，放在我手里，见他满手是泥，原来他便用这手走来的。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"里，见他满手是泥，原来他便用这手走来的。不一会，他喝完酒，便又在旁人的说笑声中，坐着用这手慢慢走去了。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 21, to: 21 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"自此以后，又长久没有看见孔乙己。到了年关，掌柜取下粉板说，“孔乙己还欠十九个钱呢！”到第二年的端午，又说“孔乙己还欠十九个钱呢！”到中秋可是没有说，再到年关也没有看见他。\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 23, to: 23 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"我到现在终于没有见——大约孔乙己的确死了。s\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 25, to: 25 } } }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { TextLoader } from \"langchain/document_loaders/fs/text\";\n",
    "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
    "const loader = new TextLoader(\"data/kong.txt\");\n",
    "const docs = await loader.load();\n",
    "\n",
    "const splitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 100,\n",
    "    chunkOverlap: 20,\n",
    "  });\n",
    "\n",
    "const splitDocs = await splitter.splitDocuments(docs);\n",
    "console.log(splitDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document {\n",
      "  pageContent: \"鲁镇的酒店的格局，是和别处不同的：都是当街一个曲尺形的大柜台，柜里面预备着热水，可以随时温酒。做工的人，傍午傍晚散了工，每每花四文铜钱，买一碗酒，——这是二十多年前的事，现在每碗要涨到十文，——靠柜外\",\n",
      "  metadata: { source: \"data/kong.txt\", loc: { lines: { from: 1, to: 1 } } }\n",
      "}\n",
      "[\n",
      "     0.017484097,   0.0005194439,    0.01524044,   -0.02138313,   -0.006707025,\n",
      "    -0.010075929,   -0.022491278, -0.0058211917,  -0.007367125,   -0.030234626,\n",
      "    -0.009323483,     0.02171147,  -0.013003625,  0.0033740338,  -0.0132361995,\n",
      "      0.01838703,    0.006600999,    0.01025378,   0.027936248,    -0.01275737,\n",
      "     -0.03143854,    0.010965182,  0.0024283468,  -0.005858814,   -0.026171422,\n",
      "       0.0185512,    0.020138176,  -0.010691565,  -0.001805869,    0.015979204,\n",
      "  0.000100147925,    0.002467679,  -0.029933648,  -0.015951844,  -0.0050755865,\n",
      "    -0.002137629, -0.00012783022, -0.0028900746,   0.016280184,  -0.0056672823,\n",
      "   0.00043821396,  -0.0075039333,    0.01489842,   0.013810794,   -0.014104932,\n",
      "     0.019604623,   0.0061803134,   0.013468773,  -0.014063889,  -0.0009858746,\n",
      "     0.006583898,   0.0084342295,  -0.030344073,   -0.00811273,   -0.008618921,\n",
      "    0.0024420274,  -0.0040666256,    0.03225939, -0.0043436624,   -0.033217046,\n",
      "    -0.011163554,    0.008037485,  -0.047937617,    0.01357822,   -0.006358164,\n",
      "     0.008482113,   -0.019235242, -0.0065667965,   0.024707573,  0.00023620801,\n",
      "      0.03332649,     0.04886791,   0.008167453,  -0.008988303,    0.028921267,\n",
      "    -0.023900403,   -0.026595525,  -0.005896436,  -0.010978864,    0.008311102,\n",
      "      0.00960394,   -0.036390997,  -0.009556057,   0.006409467,    0.016923182,\n",
      "     0.022039812,   -0.007415008,    0.03989329,  0.0008640297,    -0.03015254,\n",
      "    -0.007353444,   0.0021427595,   0.022587044,    0.01837335, -0.00023770436,\n",
      "     0.009179834,    0.030535605,     0.0185512,   0.008529996,   -0.020439155,\n",
      "  ... 1436 more items\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { load } from \"dotenv\";\n",
    "const env = await load();\n",
    "\n",
    "const process = {\n",
    "    env\n",
    "}\n",
    "\n",
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "const embeddings = new OpenAIEmbeddings()\n",
    "\n",
    "console.log(splitDocs[0])\n",
    "\n",
    "const res = await embeddings.embedQuery(splitDocs[0].pageContent)\n",
    "console.log(res);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  Document {\n",
      "    pageContent: \"有喝酒的人便都看着他笑，有的叫道，“孔乙己，你脸上又添上新伤疤了！”他不回答，对柜里说，“温两碗酒，要一碟茴香豆。”便排出九文大钱。他们又故意的高声嚷道，“你一定又偷了人家的东西了！”孔乙己睁大眼睛说\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 7, to: 7 } } }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: \"有几回，邻居孩子听得笑声，也赶热闹，围住了孔乙己。他便给他们一人一颗。孩子吃完豆，仍然不散，眼睛都望着碟子。孔乙己着了慌，伸开五指将碟子罩住，弯腰下去说道，“不多了，我已经不多了。”直起身又看一看豆\",\n",
      "    metadata: { source: \"data/kong.txt\", loc: { lines: { from: 15, to: 15 } } }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
    "\n",
    "const vectorstore = new MemoryVectorStore(embeddings);\n",
    "await vectorstore.addDocuments(splitDocs);\n",
    "\n",
    "const retriever = vectorstore.asRetriever(2)\n",
    "\n",
    "const res = await retriever.invoke(\"茴香豆是做什么用的\")\n",
    "console.log(res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文中描述了直升机试验的场景。试验过程中，直升机先停止自转，减慢下降速度直到悬停，然后再次开始下坠。直升机在下降过程中速度较快，降落点在果园里，机体倾斜，果树被压倒，螺旋桨削断了树木。机舱玻璃碎裂，驾驶员受伤。另外一架试验直升机在附近降落，机体也受损但没有大的损伤。整个过程展现了直升机在试验中的运行情况和降落状态。\n"
     ]
    }
   ],
   "source": [
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
    "import { TextLoader } from \"langchain/document_loaders/fs/text\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "\n",
    "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
    "\n",
    "import { RunnableSequence } from \"langchain/runnables\";\n",
    "\n",
    "const splitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 500,\n",
    "    chunkOverlap: 100,\n",
    "});\n",
    "\n",
    "const loader = new TextLoader(\"data/qiu.txt\");\n",
    "const docs = await loader.load();\n",
    "\n",
    "const splitDocs = await splitter.splitDocuments(docs);\n",
    "\n",
    "const embeddings = new OpenAIEmbeddings();\n",
    "\n",
    "\n",
    "const vectorstore = new MemoryVectorStore(embeddings);\n",
    "await vectorstore.addDocuments(splitDocs);\n",
    "\n",
    "const retriever = vectorstore.asRetriever(2)\n",
    "const res = await retriever.invoke(\"原文中，谁提出了宏原子的假设？并详细介绍给我宏原子假设的理论\")\n",
    "\n",
    "const convertDocsToString = (documents: Document[]): string => {\n",
    "    return documents.map((document) =>  document.pageContent).join(\"\\n\")\n",
    "}\n",
    "\n",
    "\n",
    "const contextRetriverChain = RunnableSequence.from([\n",
    "    (input) => input.question,\n",
    "    retriever,\n",
    "    convertDocsToString\n",
    "])\n",
    "\n",
    "\n",
    "const result = await contextRetriverChain.invoke({ question: \"原文中，谁提出了宏原子的假设？并详细介绍给我宏原子假设的理论\"})\n",
    "\n",
    "\n",
    "\n",
    "const TEMPLATE = `\n",
    "你是一个熟读刘慈欣的《球状闪电》的终极原著党，精通根据作品原文详细解释和回答问题，你在回答时会引用作品原文。\n",
    "并且回答时仅根据原文，尽可能回答用户问题，如果原文中没有相关内容，你可以回答“原文中没有相关内容”，\n",
    "\n",
    "以下是原文中跟用户回答相关的内容：\n",
    "{context}\n",
    "\n",
    "现在，你需要基于原文，回答以下问题：\n",
    "{question}`;\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromTemplate(\n",
    "    TEMPLATE\n",
    ");\n",
    "\n",
    "\n",
    "const model = new ChatOpenAI();\n",
    "\n",
    "const ragChain = RunnableSequence.from([\n",
    "    {\n",
    "        context: contextRetriverChain,\n",
    "        question: (input) => input.question,\n",
    "    },\n",
    "    prompt,\n",
    "    model,\n",
    "    new StringOutputParser()\n",
    "])\n",
    "\n",
    "// const answer = await ragChain.invoke({\n",
    "//     question: \"什么是球状闪电\"\n",
    "//   });  \n",
    "//   console.log(answer);\n",
    "\n",
    "const answer = await ragChain.invoke({\n",
    "    question: \"详细描述原文中有什么跟直升机相关的场景\"\n",
    "  });\n",
    "\n",
    "console.log(answer);\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  HumanMessage {\n",
      "    lc_serializable: true,\n",
      "    lc_kwargs: { content: \"hi\", additional_kwargs: {}, response_metadata: {} },\n",
      "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "    content: \"hi\",\n",
      "    name: undefined,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    lc_serializable: true,\n",
      "    lc_kwargs: {\n",
      "      content: \"What can I do for you?\",\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "    content: \"What can I do for you?\",\n",
      "    name: undefined,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "\n",
    "await history.addMessage(new HumanMessage(\"hi\"));\n",
    "await history.addMessage(new AIMessage(\"What can I do for you?\"));\n",
    "\n",
    "const messages = await history.getMessages();\n",
    "\n",
    "console.log(messages);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const chatModel = new ChatOpenAI();\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "    You are talkative and provides lots of specific details from its context. \n",
    "    If the you does not know the answer to a question, it truthfully says you do not know.`],\n",
    "    new MessagesPlaceholder(\"history_message\"),\n",
    "]);\n",
    "\n",
    "const chain = prompt.pipe(chatModel);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage {\n",
      "  lc_serializable: true,\n",
      "  lc_kwargs: {\n",
      "    content: \"Your name is Tunan. It's great to have you here! How can I help you further, Tunan?\",\n",
      "    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "  content: \"Your name is Tunan. It's great to have you here! How can I help you further, Tunan?\",\n",
      "  name: undefined,\n",
      "  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n",
      "  response_metadata: {\n",
      "    tokenUsage: { completionTokens: 25, promptTokens: 104, totalTokens: 129 },\n",
      "    finish_reason: \"stop\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "await history.addMessage(new HumanMessage(\"hi, my name is Tunan\"));\n",
    "\n",
    "const res1 = await chain.invoke({\n",
    "    history_message: await history.getMessages()\n",
    "})\n",
    "\n",
    "\n",
    "await history.addMessage(res1)\n",
    "await history.addMessage(new HumanMessage(\"What is my name?\"));\n",
    "\n",
    "const res2 = await chain.invoke({\n",
    "    history_message: await history.getMessages()\n",
    "})\n",
    "\n",
    "console.log(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"hi, my name is Kai\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"hi, my name is Kai\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  AIMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"Hello Kai! How can I assist you today?\"\u001b[39m,\n",
       "      additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"Hello Kai! How can I assist you today?\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "    response_metadata: {\n",
       "      tokenUsage: { completionTokens: \u001b[33m11\u001b[39m, promptTokens: \u001b[33m33\u001b[39m, totalTokens: \u001b[33m44\u001b[39m },\n",
       "      finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
       "    }\n",
       "  },\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  AIMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"你的名字是Kai。\"\u001b[39m,\n",
       "      additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"你的名字是Kai。\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "    response_metadata: {\n",
       "      tokenUsage: { completionTokens: \u001b[33m9\u001b[39m, promptTokens: \u001b[33m60\u001b[39m, totalTokens: \u001b[33m69\u001b[39m },\n",
       "      finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
       "    }\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "\n",
    "const chatModel = new ChatOpenAI();\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"],\n",
    "    new MessagesPlaceholder(\"history_message\"),\n",
    "    [\"human\",\"{input}\"]\n",
    "]);\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "const chain = prompt.pipe(chatModel)\n",
    "\n",
    "const chainWithHistory = new RunnableWithMessageHistory({\n",
    "  runnable: chain,\n",
    "  getMessageHistory: (_sessionId) => history,\n",
    "  inputMessagesKey: \"input\",\n",
    "  historyMessagesKey: \"history_message\",\n",
    "});\n",
    "\n",
    "const res1 = await chainWithHistory.invoke({\n",
    "    input: \"hi, my name is Kai\"\n",
    "},{\n",
    "    configurable: { sessionId: \"none\" }\n",
    "})\n",
    "\n",
    "\n",
    "const res2 = await chainWithHistory.invoke({\n",
    "    input: \"我的名字叫什么？\"\n",
    "},{\n",
    "    configurable: { sessionId: \"none\" }\n",
    "})\n",
    "\n",
    "\n",
    "await history.getMessages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"The individual is an 18-year-old male.\"\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { RunnableSequence } from \"langchain/runnables\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { RunnablePassthrough } from \"@langchain/core/runnables\";\n",
    "\n",
    "const summaryModel = new ChatOpenAI();\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "New lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:\n",
    "`); \n",
    "\n",
    "const summaryChain = RunnableSequence.from([\n",
    "    summaryPrompt,\n",
    "    summaryModel,\n",
    "    new StringOutputParser(),\n",
    "])\n",
    "\n",
    "const newSummary = await summaryChain.invoke({\n",
    "    summary: \"\",\n",
    "    new_lines: \"I'm 18\"\n",
    "})\n",
    "\n",
    "await summaryChain.invoke({\n",
    "    summary: newSummary,\n",
    "    new_lines: \"I'm male\"\n",
    "})\n",
    "\n",
    "const chatModel = new ChatOpenAI();\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "\n",
    "    Here is the chat history summary:\n",
    "    {history_summary}\n",
    "    `],\n",
    "    [\"human\",\"{input}\"]\n",
    "]);\n",
    "let summary = \"\"\n",
    "const history = new ChatMessageHistory();\n",
    "\n",
    "\n",
    "const chatChain = RunnableSequence.from([\n",
    "    {\n",
    "        input: new RunnablePassthrough({\n",
    "             func: (input) => history.addUserMessage(input)\n",
    "        })\n",
    "    },\n",
    "    RunnablePassthrough.assign({\n",
    "        history_summary: () => summary\n",
    "    }),\n",
    "    chatPrompt,\n",
    "    chatModel,\n",
    "    new StringOutputParser(),\n",
    "    new RunnablePassthrough({\n",
    "        func: async (input) => {\n",
    "            history.addAIChatMessage(input)\n",
    "            const messages = await history.getMessages()\n",
    "            const new_lines = getBufferString(messages)\n",
    "            const newSummary = await summaryChain.invoke({\n",
    "                summary,\n",
    "                new_lines\n",
    "            })\n",
    "            history.clear()\n",
    "            summary = newSummary      \n",
    "        }\n",
    "    })\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"如果你想吃方便面，你可以快速地泡一碗方便面来满足你的食欲。另外，你也可以考虑配一些水果、酸奶、面包或饼干，让餐点更加丰富。如果需要的话，也可以点外卖或送餐服务来享用不同口味的美食。希望这些建议能帮助你解决饥饿问题。\"\u001b[39m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { RunnableMap } from \"@langchain/core/runnables\"\n",
    "import { getBufferString } from \"@langchain/core/messages\"\n",
    "\n",
    "const mapChain = RunnableMap.from({\n",
    "    a: () => \"a\",\n",
    "    b: () => \"b\"\n",
    "})\n",
    "\n",
    "const res = await mapChain.invoke()\n",
    "// { a: \"a\", b: \"b\" }\n",
    "\n",
    "await chatChain.invoke(\"我现在饿了\")\n",
    "\n",
    "await chatChain.invoke(\"我今天想吃方便面\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { BufferMemory } from \"langchain/memory\";\n",
    "import { ConversationChain } from \"langchain/chains\";\n",
    "\n",
    "\n",
    "const chatModel = new ChatOpenAI();\n",
    "const memory = new BufferMemory();\n",
    "const chain = new ConversationChain({ llm: chatModel, memory: memory });\n",
    "const res1 = await chain.call({ input: \"我是小明\" });\n",
    "\n",
    "\n",
    "const res2 = await chain.call({ input: \"我叫什么？\" });\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[chain/start]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] Entering Chain run with input: {\n",
      "  \"input\": \"我是小明\",\n",
      "  \"summary\": \"\"\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"\\n你是一个乐于助人的助手。尽你所能回答所有问题。\\n\\n这是聊天记录的摘要:\\n\\nHuman: 我是小明\\nAI:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] [1.19s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" 你好，小明！我是你的专属助手，有什么可以帮助你的呢？\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \" 你好，小明！我是你的专属助手，有什么可以帮助你的呢？\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 33,\n",
      "                \"promptTokens\": 54,\n",
      "                \"totalTokens\": 87\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 33,\n",
      "      \"promptTokens\": 54,\n",
      "      \"totalTokens\": 87\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\n\\nNew lines of conversation:\\nHuman: 我是小明\\nAI:  你好，小明！我是你的专属助手，有什么可以帮助你的呢？\\n\\nNew summary:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [774ms] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human introduces himself as Xiaoming. The AI responds by greeting Xiaoming and offering assistance as his personal assistant.\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human introduces himself as Xiaoming. The AI responds by greeting Xiaoming and offering assistance as his personal assistant.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 24,\n",
      "                \"promptTokens\": 177,\n",
      "                \"totalTokens\": 201\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 24,\n",
      "      \"promptTokens\": 177,\n",
      "      \"totalTokens\": 201\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] [1.97s] Exiting Chain run with output: {\n",
      "  \"response\": \" 你好，小明！我是你的专属助手，有什么可以帮助你的呢？\"\n",
      "}\n",
      "\u001b[32m[chain/start]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] Entering Chain run with input: {\n",
      "  \"input\": \"我叫什么？\",\n",
      "  \"summary\": \"The human introduces himself as Xiaoming. The AI responds by greeting Xiaoming and offering assistance as his personal assistant.\"\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"\\n你是一个乐于助人的助手。尽你所能回答所有问题。\\n\\n这是聊天记录的摘要:\\nThe human introduces himself as Xiaoming. The AI responds by greeting Xiaoming and offering assistance as his personal assistant.\\nHuman: 我叫什么？\\nAI:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] [757ms] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" 你说你叫xiaoming。\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \" 你说你叫xiaoming。\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 10,\n",
      "                \"promptTokens\": 79,\n",
      "                \"totalTokens\": 89\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 10,\n",
      "      \"promptTokens\": 79,\n",
      "      \"totalTokens\": 89\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\nThe human introduces himself as Xiaoming. The AI responds by greeting Xiaoming and offering assistance as his personal assistant.\\n\\nNew lines of conversation:\\nHuman: 我叫什么？\\nAI:  你说你叫xiaoming。\\n\\nNew summary:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [1.23s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The human, Xiaoming, introduces himself and asks what his name is. The AI responds in Chinese, confirming that his name is Xiaoming.\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The human, Xiaoming, introduces himself and asks what his name is. The AI responds in Chinese, confirming that his name is Xiaoming.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 30,\n",
      "                \"promptTokens\": 179,\n",
      "                \"totalTokens\": 209\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 30,\n",
      "      \"promptTokens\": 179,\n",
      "      \"totalTokens\": 209\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] [1.99s] Exiting Chain run with output: {\n",
      "  \"response\": \" 你说你叫xiaoming。\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ConversationSummaryMemory } from \"langchain/memory\";\n",
    "import { PromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "const memory = new ConversationSummaryMemory({\n",
    "    memoryKey: \"summary\",\n",
    "    llm: new ChatOpenAI({\n",
    "          verbose: true,\n",
    "    }),\n",
    "  });\n",
    "\n",
    "const model = new ChatOpenAI();\n",
    "const prompt = PromptTemplate.fromTemplate(`\n",
    "你是一个乐于助人的助手。尽你所能回答所有问题。\n",
    "\n",
    "这是聊天记录的摘要:\n",
    "{summary}\n",
    "Human: {input}\n",
    "AI:`);\n",
    "const chain = new ConversationChain({ llm: model, prompt, memory, verbose: true });\n",
    "\n",
    "const res1 = await chain.call({ input: \"我是小明\" });\n",
    "const res2 = await chain.call({ input: \"我叫什么？\" });\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \\\"What do you know about him\\\" where \\\"him\\\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: my name is Jacob. how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Jacob,Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\n\\nLast line of conversation (for extraction):\\nHuman: 我叫小明，今年 18 岁\\n\\nOutput:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [601ms] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"小明\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"小明\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 3,\n",
      "                \"promptTokens\": 419,\n",
      "                \"totalTokens\": 422\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 3,\n",
      "      \"promptTokens\": 419,\n",
      "      \"totalTokens\": 422\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[chain/start]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] Entering Chain run with input: {\n",
      "  \"input\": \"我叫小明，今年 18 岁\",\n",
      "  \"history\": \"\",\n",
      "  \"entities\": {\n",
      "    \"小明\": \"No current information known.\"\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an assistant to a human, powered by a large language model trained by OpenAI.\\n\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\n\\nContext:\\n[object Object]\\n\\nCurrent conversation:\\n\\nLast line:\\nHuman: 我叫小明，今年 18 岁\\nYou:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] [814ms] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Nice to meet you, Xiao Ming! If you have any questions or need assistance, feel free to ask.\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Nice to meet you, Xiao Ming! If you have any questions or need assistance, feel free to ask.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 23,\n",
      "                \"promptTokens\": 284,\n",
      "                \"totalTokens\": 307\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 23,\n",
      "      \"promptTokens\": 284,\n",
      "      \"totalTokens\": 307\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update and add to the summary of the provided entity in the \\\"Entity\\\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), output the exact string \\\"UNCHANGED\\\" below.\\n\\nFull conversation history (for context):\\nHuman: 我叫小明，今年 18 岁\\nAI: Nice to meet you, Xiao Ming! If you have any questions or need assistance, feel free to ask.\\n\\nEntity to summarize:\\n小明\\n\\nExisting summary of 小明:\\nNo current information known.\\n\\nLast line of conversation:\\nHuman: 我叫小明，今年 18 岁\\nUpdated summary (or the exact string \\\"UNCHANGED\\\" if there is no new information about 小明 above):\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [666ms] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Xiao Ming is 18 years old.\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Xiao Ming is 18 years old.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 10,\n",
      "                \"promptTokens\": 255,\n",
      "                \"totalTokens\": 265\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 10,\n",
      "      \"promptTokens\": 255,\n",
      "      \"totalTokens\": 265\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] [1.48s] Exiting Chain run with output: {\n",
      "  \"response\": \"Nice to meet you, Xiao Ming! If you have any questions or need assistance, feel free to ask.\"\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \\\"What do you know about him\\\" where \\\"him\\\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: my name is Jacob. how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Jacob,Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how's it going today?\\nAI: \\\"It's going great! How about you?\\\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \\\"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\\\"\\nLast line:\\nPerson #1: i'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\nHuman: 我叫小明，今年 18 岁\\nAI: Nice to meet you, Xiao Ming! If you have any questions or need assistance, feel free to ask.\\nLast line of conversation (for extraction):\\nHuman: ABC 是一家互联网公司，主要是售卖方便面的公司\\n\\nOutput:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [614ms] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"ABC, 互联网公司, 方便面\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"ABC, 互联网公司, 方便面\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 14,\n",
      "                \"promptTokens\": 469,\n",
      "                \"totalTokens\": 483\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 14,\n",
      "      \"promptTokens\": 469,\n",
      "      \"totalTokens\": 483\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[chain/start]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] Entering Chain run with input: {\n",
      "  \"input\": \"ABC 是一家互联网公司，主要是售卖方便面的公司\",\n",
      "  \"history\": \"Human: 我叫小明，今年 18 岁\\nAI: Nice to meet you, Xiao Ming! If you have any questions or need assistance, feel free to ask.\",\n",
      "  \"entities\": {\n",
      "    \"ABC\": \"No current information known.\",\n",
      "    \"互联网公司\": \"No current information known.\",\n",
      "    \"方便面\": \"No current information known.\"\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an assistant to a human, powered by a large language model trained by OpenAI.\\n\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\n\\nContext:\\n[object Object]\\n\\nCurrent conversation:\\nHuman: 我叫小明，今年 18 岁\\nAI: Nice to meet you, Xiao Ming! If you have any questions or need assistance, feel free to ask.\\nLast line:\\nHuman: ABC 是一家互联网公司，主要是售卖方便面的公司\\nYou:\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m1:chain:ConversationChain > \u001b[1m2:llm:ChatOpenAI\u001b[22m\u001b[39m] [949ms] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"That's interesting! ABC sounds like a unique company focused on selling instant noodles. Do you have any specific questions or topics you'd like to discuss further about ABC or the company's industry?\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"That's interesting! ABC sounds like a unique company focused on selling instant noodles. Do you have any specific questions or topics you'd like to discuss further about ABC or the company's industry?\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 39,\n",
      "                \"promptTokens\": 334,\n",
      "                \"totalTokens\": 373\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 39,\n",
      "      \"promptTokens\": 334,\n",
      "      \"totalTokens\": 373\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update and add to the summary of the provided entity in the \\\"Entity\\\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), output the exact string \\\"UNCHANGED\\\" below.\\n\\nFull conversation history (for context):\\nHuman: 我叫小明，今年 18 岁\\nAI: Nice to meet you, Xiao Ming! If you have any questions or need assistance, feel free to ask.\\nHuman: ABC 是一家互联网公司，主要是售卖方便面的公司\\nAI: That's interesting! ABC sounds like a unique company focused on selling instant noodles. Do you have any specific questions or topics you'd like to discuss further about ABC or the company's industry?\\n\\nEntity to summarize:\\nABC\\n\\nExisting summary of ABC:\\nNo current information known.\\n\\nLast line of conversation:\\nHuman: ABC 是一家互联网公司，主要是售卖方便面的公司\\nUpdated summary (or the exact string \\\"UNCHANGED\\\" if there is no new information about ABC above):\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [549ms] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"ABC is an internet company that primarily sells instant noodles.\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"ABC is an internet company that primarily sells instant noodles.\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 12,\n",
      "                \"promptTokens\": 326,\n",
      "                \"totalTokens\": 338\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 12,\n",
      "      \"promptTokens\": 326,\n",
      "      \"totalTokens\": 338\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update and add to the summary of the provided entity in the \\\"Entity\\\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), output the exact string \\\"UNCHANGED\\\" below.\\n\\nFull conversation history (for context):\\nHuman: 我叫小明，今年 18 岁\\nAI: Nice to meet you, Xiao Ming! If you have any questions or need assistance, feel free to ask.\\nHuman: ABC 是一家互联网公司，主要是售卖方便面的公司\\nAI: That's interesting! ABC sounds like a unique company focused on selling instant noodles. Do you have any specific questions or topics you'd like to discuss further about ABC or the company's industry?\\n\\nEntity to summarize:\\n互联网公司\\n\\nExisting summary of 互联网公司:\\nNo current information known.\\n\\nLast line of conversation:\\nHuman: ABC 是一家互联网公司，主要是售卖方便面的公司\\nUpdated summary (or the exact string \\\"UNCHANGED\\\" if there is no new information about 互联网公司 above):\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [559ms] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"互联网公司主要是售卖方便面。\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"互联网公司主要是售卖方便面。\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 18,\n",
      "                \"promptTokens\": 340,\n",
      "                \"totalTokens\": 358\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 18,\n",
      "      \"promptTokens\": 340,\n",
      "      \"totalTokens\": 358\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain_core\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update and add to the summary of the provided entity in the \\\"Entity\\\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), output the exact string \\\"UNCHANGED\\\" below.\\n\\nFull conversation history (for context):\\nHuman: 我叫小明，今年 18 岁\\nAI: Nice to meet you, Xiao Ming! If you have any questions or need assistance, feel free to ask.\\nHuman: ABC 是一家互联网公司，主要是售卖方便面的公司\\nAI: That's interesting! ABC sounds like a unique company focused on selling instant noodles. Do you have any specific questions or topics you'd like to discuss further about ABC or the company's industry?\\n\\nEntity to summarize:\\n方便面\\n\\nExisting summary of 方便面:\\nNo current information known.\\n\\nLast line of conversation:\\nHuman: ABC 是一家互联网公司，主要是售卖方便面的公司\\nUpdated summary (or the exact string \\\"UNCHANGED\\\" if there is no new information about 方便面 above):\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"response_metadata\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:ChatOpenAI\u001b[22m\u001b[39m] [793ms] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"方便面是ABC公司主要销售的产品。\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain_core\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"方便面是ABC公司主要销售的产品。\",\n",
      "            \"additional_kwargs\": {},\n",
      "            \"response_metadata\": {\n",
      "              \"tokenUsage\": {\n",
      "                \"completionTokens\": 16,\n",
      "                \"promptTokens\": 335,\n",
      "                \"totalTokens\": 351\n",
      "              },\n",
      "              \"finish_reason\": \"stop\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"generationInfo\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llmOutput\": {\n",
      "    \"tokenUsage\": {\n",
      "      \"completionTokens\": 16,\n",
      "      \"promptTokens\": 335,\n",
      "      \"totalTokens\": 351\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[36m[chain/end]\u001b[39m [\u001b[90m\u001b[1m1:chain:ConversationChain\u001b[22m\u001b[39m] [2.85s] Exiting Chain run with output: {\n",
      "  \"response\": \"That's interesting! ABC sounds like a unique company focused on selling instant noodles. Do you have any specific questions or topics you'd like to discuss further about ABC or the company's industry?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { EntityMemory, ENTITY_MEMORY_CONVERSATION_TEMPLATE } from \"langchain/memory\";\n",
    "import { ConversationChain } from \"langchain/chains\";\n",
    "\n",
    "const model = new ChatOpenAI();\n",
    "const memory = new EntityMemory({\n",
    "    llm: new ChatOpenAI({\n",
    "        verbose: true \n",
    "    }),\n",
    "    chatHistoryKey: \"history\",\n",
    "    entitiesKey: \"entities\"\n",
    "});\n",
    "const chain = new ConversationChain({ \n",
    "    llm: model, \n",
    "    prompt: ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory: memory, \n",
    "    verbose: true \n",
    "});\n",
    "\n",
    "const res1 = await chain.call({ input: \"我叫小明，今年 18 岁\" });\n",
    "const res2 = await chain.call({ input: \"ABC 是一家互联网公司，主要是售卖方便面的公司\" });\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Cannot use 'import.meta' outside a module",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "SyntaxError: Cannot use 'import.meta' outside a module"
     ]
    }
   ],
   "source": [
    " import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    " import { RunnableSequence } from \"@langchain/core/runnables\";\n",
    " import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "import path from 'node:path';\n",
    "import { fileURLToPath } from 'node:url';\n",
    "\n",
    "const __filename = fileURLToPath(import.meta.url);\n",
    "const __dirname = path.dirname(__filename);\n",
    "\n",
    "\n",
    " const baseDir = __dirname;\n",
    " const rephraseChainPrompt = ChatPromptTemplate.fromMessages([\n",
    "    [\n",
    "      \"system\",\n",
    "      \"给定以下对话和一个后续问题，请将后续问题重述为一个独立的问题。请注意，重述的问题应该包含足够的信息，使得没有看过对话历史的人也能理解。\",\n",
    "    ],\n",
    "    new MessagesPlaceholder(\"history\"),\n",
    "    [\"human\", \"将以下问题重述为一个独立的问题：\\n{question}\"],\n",
    "  ]);\n",
    "\n",
    "   const rephraseChain = RunnableSequence.from([\n",
    "    rephraseChainPrompt,\n",
    "    new ChatOpenAI({\n",
    "      temperature: 0.2,\n",
    "    }),\n",
    "    new StringOutputParser(),\n",
    "  ]);\n",
    "\n",
    "  const historyMessages = [new HumanMessage(\"你好，我叫小明\"), new AIMessage(\"你好小明\")];\n",
    "  \n",
    "  const question = \"你觉得我的名字怎么样？\";\n",
    "  const standaloneQuestion = await rephraseChain.invoke({ history: historyMessages, question });\n",
    "\n",
    "  console.log(standaloneQuestion);\n",
    "  // 你认为小明这个名字怎么样？\n",
    "\n",
    "\n",
    "  const loader = new TextLoader(path.join(baseDir, \"../../data/qiu.txt\"));\n",
    "  const docs = await loader.load();\n",
    "\n",
    "  const splitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 500,\n",
    "    chunkOverlap: 100,\n",
    "  });\n",
    "\n",
    "  const splitDocs = await splitter.splitDocuments(docs);\n",
    "\n",
    "  const embeddings = new OpenAIEmbeddings();\n",
    "  const vectorStore = await FaissStore.fromDocuments(splitDocs, embeddings);\n",
    "\n",
    "  await vectorStore.save(path.join(baseDir, \"../../db/qiu\"));\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
